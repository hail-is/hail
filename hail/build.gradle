buildscript {
    repositories {
        mavenCentral()
        jcenter()
    }
}

plugins {
  id "com.gradle.build-scan" version "2.3"
  id 'java'
  id 'scala'
  id 'idea'
  id 'maven'
  id 'jacoco'
  id 'com.github.johnrengelman.shadow' version '5.0.0'
  id "de.undercouch.download" version "3.2.0"
  id 'eclipse'
}

import com.github.jengelman.gradle.plugins.shadow.tasks.ShadowJar

repositories {
    mavenCentral()
    jcenter()
    maven { url "https://repository.cloudera.com/artifactory/cloudera-repos/" }
    maven { url "http://repo.hortonworks.com/content/repositories/releases/" }
    maven { url "http://repo.spring.io/plugins-release/" }
}

buildScan {
    termsOfServiceUrl = 'https://gradle.com/terms-of-service'
    termsOfServiceAgree = 'yes'
}

// Hail has three notions of version:
//  - hail short version: MAJOR.MINOR
//  - hail pip version: MAJOR.MINOR.PATCH
//  - hail version: MAJOR.MINOR.PATCH-GIT_SHA, calculated by generateBuildInfo
String MAJOR = 0
String MINOR = 2
String PATCH = 14
String hailShortVersion = MAJOR.toString() + "." + MINOR.toString()
// By default pip hides versions ending in ".devN". Use these for testing.
String hailPipVersion = hailShortVersion + "." + PATCH + System.getProperty("hail.pip-version-suffix", "")
String[] deployedSparkVersions = new File("deployed-spark-versions.txt").readLines()

String sparkVersion = System.getProperty("spark.version", "2.4.0")
String breezeVersion = System.getProperty("breeze.version")
String py4jVersion = System.getProperty("py4j.version")

if (!(sparkVersion ==~ /^2\.[2-4].*/))
    ant.fail('Hail does not support Spark version ' + sparkVersion + '. Hail team recommends version 2.4.0.')

String scalaVersion = '2.11.8'
String scalaMajorVersion = '2.11'

if (py4jVersion == null) {
  if (sparkVersion == '2.2.0')
    py4jVersion = '0.10.4'
  else if (sparkVersion == '2.3.0')
    py4jVersion = '0.10.6'
  else
    py4jVersion = '0.10.7'
}

if (breezeVersion == null) {
  if (sparkVersion == '2.2.0')
    breezeVersion = '0.13.1'
  else
    breezeVersion = '0.13.2'
}

String sparkHome = System.getProperty("spark.home", System.env.SPARK_HOME)

String parallelism = System.getProperty("test.parallelism", "2")

sourceSets.main.scala.srcDir "src/main/java"
sourceSets.main.java.srcDirs = []
sourceSets.test.runtimeClasspath += files("prebuilt/lib")

task nativeLib(type: Exec) {
    workingDir 'src/main/c'
    args('-j'+parallelism)
    executable 'make'
    outputs.upToDateWhen { false }
}

task nativeLibTest(type: Exec) {
    workingDir 'src/main/c'
    args('-j'+parallelism, 'test')
    executable 'make'
    outputs.upToDateWhen { false }
}

task nativeLibClean(type: Exec) {
    workingDir 'src/main/c'
    args('clean')
    executable 'make'
    outputs.upToDateWhen { false }
}

task nativeLibPrebuilt(type: Exec) {
    workingDir 'src/main/c'
    args('prebuilt')
    executable 'make'
    outputs.upToDateWhen { false }
}

task nativeLibResetPrebuilt(type: Exec) {
    workingDir 'src/main/c'
    args('reset-prebuilt')
    executable 'make'
    outputs.upToDateWhen { false }
}

sourceSets {
    main {
        resources {
            srcDirs "prebuilt/lib"
        }
    }
}
compileScala.dependsOn(nativeLib)
clean.dependsOn(nativeLibClean)

task testCppCodegen(type: Test) {
    useTestNG {}


    // Enable C++ codegen for tests
    environment HAIL_ENABLE_CPP_CODEGEN: '1'

    filter {
        includeTestsMatching "is.hail.nativecode.NativeCodeSuite"
        includeTestsMatching "is.hail.annotations.AnnotationsSuite.testReadWrite"
        includeTestsMatching "is.hail.annotations.UnsafeSuite"
        includeTestsMatching "is.hail.expr.ir.TableIRSuite.testShuffleAndJoinDoesntMemoryLeak"
        includeTestsMatching "is.hail.io.IndexSuite"
    }

    // avoid stack overflow in lmmLargeExampleTest on some systems
    jvmArgs '-Xss4m'

    systemProperties System.getProperties()

    testLogging {
        events "passed", "skipped", "failed"
    }

    // listen to events in the test execution lifecycle
    beforeTest { descriptor ->
        logger.lifecycle("Running test: " + descriptor)
    }

    // make poop emoji work in generated bytecode
    systemProperty "file.encoding", "utf-8"
}

testCppCodegen.reports.html.destination = file("$buildDir/reports/codegen-tests")

compileJava {
    options.compilerArgs << "-Xlint:all" << "-Werror" << "-XDenableSunApiLintControl"
}
tasks.withType(JavaCompile) {
    options.fork = true // necessary to make -XDenableSunApiLintControl work
}


task generateBuildInfo(type: Exec) {
    commandLine 'bash', 'generate-build-info.sh', sparkVersion, hailPipVersion
    outputs.upToDateWhen { false }
}


task pipInstall(type: Exec, dependsOn: ['generateBuildInfo', 'shadowJar']) {
    commandLine 'bash', 'python/pipinstall.sh'
    outputs.upToDateWhen { false }
}

shadowJar.mustRunAfter nativeLibPrebuilt
nativeLibResetPrebuilt.mustRunAfter shadowJar
task releaseJar(type: Wrapper, dependsOn: ['nativeLibPrebuilt', 'shadowJar', 'nativeLibResetPrebuilt']) {
}

compileScala {
    dependsOn generateBuildInfo
    scalaCompileOptions.additionalParameters = [
        "-feature",
        "-Xno-patmat-analysis",
        "-Xfatal-warnings",
        "-Xlint:_",
        "-deprecation",
        "-unchecked",
        "-Xlint:-infer-any",
        "-Xlint:-unsound-match"
    ]
    scalaCompileOptions.forkOptions.with {
        jvmArgs = ["-Xms512M",
                   "-Xmx4096M",
                   "-Xss4M",
                   "-XX:MaxMetaspaceSize=1024M"]
    }
}

configurations {
    compile.extendsFrom bundled, unbundled
    testCompile.extendsFrom compile, hailTest
    hailJar.extendsFrom bundled
    hailTestJar.extendsFrom hailJar, hailTest
}

dependencies {
    unbundled 'org.scala-lang:scala-library:' + scalaVersion
    unbundled 'org.scala-lang:scala-reflect:' + scalaVersion
    unbundled('org.apache.spark:spark-core_' + scalaMajorVersion + ':' + sparkVersion) {
        exclude module: 'hadoop-client'
    }
    unbundled('org.apache.hadoop:hadoop-client:2.7.1') {
        exclude module: 'servlet-api'
    }
    unbundled 'org.apache.spark:spark-sql_' + scalaMajorVersion + ':' + sparkVersion
    unbundled 'org.apache.spark:spark-mllib_' + scalaMajorVersion + ':' + sparkVersion
    bundled 'org.lz4:lz4-java:1.4.0'
    bundled 'org.scalanlp:breeze-natives_' + scalaMajorVersion + ':' + breezeVersion
    bundled 'com.github.samtools:htsjdk:2.18.0'

    bundled group: 'org.slf4j', name: 'slf4j-api', version: '1.7.25'

    bundled 'org.http4s:http4s-core_' + scalaMajorVersion + ':0.12.3'
    bundled 'org.http4s:http4s-server_' + scalaMajorVersion + ':0.12.3'
    bundled 'org.http4s:http4s-argonaut_' + scalaMajorVersion + ':0.12.3'
    bundled 'org.http4s:http4s-dsl_' + scalaMajorVersion + ':0.12.3'
    bundled 'org.http4s:http4s-scala-xml_' + scalaMajorVersion + ':0.12.3'
    bundled 'org.http4s:http4s-client_' + scalaMajorVersion + ':0.12.3'
    bundled 'org.http4s:http4s-websocket_' + scalaMajorVersion + ':0.1.3'
    bundled 'org.http4s:http4s-blaze-core_' + scalaMajorVersion + ':0.12.3'
    bundled 'org.http4s:http4s-blaze-client_' + scalaMajorVersion + ':0.12.3'
    bundled 'org.http4s:http4s-blaze-server_' + scalaMajorVersion + ':0.12.3'

    bundled 'org.elasticsearch:elasticsearch-spark-20_2.11:6.2.4'

    bundled 'com.jayway.restassured:rest-assured:2.8.0'

    bundled group: 'org.ow2.asm', name: 'asm', version: '5.1'
    bundled group: 'org.ow2.asm', name: 'asm-util', version: '5.1'
    bundled group: 'org.ow2.asm', name: 'asm-analysis', version: '5.1'

    bundled 'net.java.dev.jna:jna:4.2.2'
    bundled 'net.sourceforge.jdistlib:jdistlib:0.4.5'

    hailTest 'org.testng:testng:6.8.21'
    hailTest 'org.scalatest:scalatest_' + scalaMajorVersion + ':2.2.4'

    bundled group: 'org.apache.commons', name: 'commons-math3', version: '3.6.1'
    bundled group: 'commons-codec', name: 'commons-codec', version: '1.11'
}

task(checkSettings) doLast {
    def checkSeed = System.getProperty("check.seed", "1")
    if (checkSeed == "random")
        checkSeed = new Random().nextInt().toString()
    def checkSize = System.getProperty("check.size", "1000")
    def checkCount = System.getProperty("check.count", "10")

    println "check: seed = $checkSeed, size = $checkSize, count = $checkCount"

    // override with these defaults, random seed
    System.setProperty("check.seed", checkSeed)
    System.setProperty("check.size", checkSize)
    System.setProperty("check.count", checkCount)
}

test {
    useTestNG {}

    // avoid stack overflow in lmmLargeExampleTest on some systems
    jvmArgs '-Xss4m'

    systemProperties System.getProperties()

    testLogging {
        events "passed", "skipped", "failed"
    }

    // listen to events in the test execution lifecycle
    beforeTest { descriptor ->
        logger.lifecycle("Running test: " + descriptor)
    }

    maxParallelForks parallelism.toInteger()

    // make poop emoji work in generated bytecode
    systemProperty "file.encoding", "utf-8"
}

test.dependsOn(checkSettings)

task testPython(type: Exec, dependsOn: shadowJar) {
    commandLine('pytest',
            '-v',
            '-n',
            parallelism,
            '--dist=loadscope',
            '--noconftest',
            '--color=no',
            '-r a',
            '--html=build/reports/pytest.html',
            '--self-contained-html',
            *(System.env.PYTEST_ARGS == null ? "" : System.env.PYTEST_ARGS).split(" "),
            'python/test')
    environment SPARK_HOME: sparkHome
    environment PYTHONPATH: '' + projectDir + '/python:' + sparkHome + '/python:' + sparkHome + '/python/lib/py4j-' + py4jVersion + '-src.zip'
    environment PYSPARK_SUBMIT_ARGS: '--conf spark.driver.extraClassPath=' + projectDir + '/build/libs/hail-all-spark.jar --conf spark.executor.extraClassPath=' + projectDir + '/build/libs/hail-all-spark.jar pyspark-shell'
    environment PYSPARK_PYTHON: 'python3'
    environment HAIL_TEST_RESOURCES_DIR: '' + projectDir + '/src/test/resources'
    environment HAIL_DOCTEST_DATA_DIR: '' + projectDir + '/python/hail/docs/data'
}

task doctest(type: Exec, dependsOn: shadowJar) {
    commandLine 'sh', 'python/hail/docs/doctest.sh', '-n', parallelism
    environment SPARK_HOME: sparkHome
    environment PYTHONPATH: '' + projectDir + '/python:' + sparkHome + '/python:' + sparkHome + '/python/lib/py4j-' + py4jVersion + '-src.zip'
    environment PYSPARK_SUBMIT_ARGS: '--conf spark.driver.extraClassPath=' + projectDir + '/build/libs/hail-all-spark.jar --conf spark.executor.extraClassPath=' + projectDir + '/build/libs/hail-all-spark.jar pyspark-shell'
    environment PYSPARK_PYTHON: 'python3'
    environment HAIL_TEST_RESOURCES_DIR: '' + projectDir + '/src/test/resources'
    environment HAIL_DOCTEST_DATA_DIR: '' + projectDir + '/python/hail/docs/data'
}

task benchmark(type: Exec, dependsOn: shadowJar) {
    commandLine 'python', '-m', 'benchmark'
    environment PYTHONPATH: '' + projectDir + '/python:' + sparkHome + '/python:' + sparkHome + '/python/lib/py4j-' + py4jVersion + '-src.zip'
}

task testAll(dependsOn: ['testPython', 'test', 'doctest', 'testCppCodegen'])

tasks.withType(ShadowJar) {
    manifest {
        attributes 'Implementation-Title': 'Hail',
                'Implementation-Version': '0.0.1-SNAPSHOT'
    }
    baseName = project.name + '-all'
    mergeServiceFiles()
    zip64 true
    // conflict with version in default Hadoop/Spark install
    relocate 'org.apache.http', 'is.hail.relocated.org.apache.http'
    relocate 'com.google.common', 'is.hail.relocated.com.google.common'
    relocate 'org.objectweb', 'is.hail.relocated.org.objectweb'

    exclude 'META-INF/*.RSA'
    exclude 'META-INF/*.SF'
    exclude 'META-INF/*.DSA'
}

shadowJar {
    classifier = 'spark'
    from project.sourceSets.main.output
    configurations = [project.configurations.hailJar]
}

task shadowTestJar(type: ShadowJar) {
    classifier = 'spark-test'
    from project.sourceSets.main.output, project.sourceSets.test.output
    configurations = [project.configurations.hailTestJar]
}

jacocoTestReport {
    dependsOn test
    reports {
        xml.enabled false
        csv.enabled false
        html.destination file("${buildDir}/reports/coverage")
    }
}

task downloadDependencies(type: Exec) {
    configurations.testCompile.files
    configurations.testRuntime.files
    commandLine 'echo', 'Downloaded all dependencies'
}

task coverage(dependsOn: jacocoTestReport)

task testJar(type: Jar) {
    classifier = 'tests'
    from sourceSets.test.output
}

task archiveZip(type: Zip, dependsOn: generateBuildInfo) {
    from fileTree('python')
    classifier = 'python'
}

task cleanDocs(type: Exec) {
    commandLine 'rm', '-rf', 'build/www/', 'build/tmp/python', 'build/tmp/docs'
}

task makeDocs(type: Exec, dependsOn: ['shadowJar', 'generateBuildInfo']) {
    commandLine 'bash', 'python/hail/docs/makeDocs.sh'
    environment SPARK_HOME: sparkHome
    environment PYSPARK_SUBMIT_ARGS: '--master local[4] --conf spark.driver.extraClassPath=' + projectDir + '/build/libs/hail-all-spark.jar --conf spark.executor.extraClassPath=' + projectDir + '/build/libs/hail-all-spark.jar pyspark-shell'
    environment PYTHONPATH: '' + projectDir + '/python:' + sparkHome + '/python:' + sparkHome + '/python/lib/py4j-' + py4jVersion + '-src.zip'
    environment HAIL_SHORT_VERSION: hailShortVersion
    environment SPHINXOPTS: '-tchecktutorial'
}

task makeDocsNoTest(type: Exec, dependsOn: ['generateBuildInfo']) {
    commandLine 'bash', 'python/hail/docs/makeDocs.sh'
    environment SPARK_HOME: sparkHome
    // makeDocsNoTest does not try to run the notebook contents
    // so we do not need the PySpark args.
    environment PYTHONPATH: '' + projectDir + '/python:' + sparkHome + '/python:' + sparkHome + '/python/lib/py4j-' + py4jVersion + '-src.zip'
    environment HAIL_SHORT_VERSION: hailShortVersion
    environment SPHINXOPTS: ''
}

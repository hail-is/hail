from dataclasses import dataclass, field
import json
from typing import Dict, List, Optional, Union

from hailtop.aiotools.fs import AsyncFS
import hailtop.batch as hb

from .config import CheckpointConfigMixin, JobConfigMixin
from .constants import SaigeAnalysisType, SaigeInputDataType, SaigeTestType, saige_phenotype_to_test_type
from .io import (
    HailTableResourceFile,
    PlinkResourceGroup,
    SaigeGeneGLMMResourceGroup,
    SaigeGLMMResourceGroup,
    SaigeSparseGRMResourceGroup,
    SaigeGeneResultResourceGroup,
    SaigeResultResourceGroup,
    TextResourceFile,
    checkpoint_if_requested,
    load_saige_glmm_file,
    load_saige_result_file,
    load_saige_sparse_grm_file,
    load_text_file,
    new_hail_table,
    new_saige_glmm_file,
    new_saige_result_file,
    new_saige_sparse_grm_file,
    new_text_file,
)
from .phenotype import Phenotype, PhenotypeConfig
from .variant_chunk import VariantChunk


def get_output_dir(config: CheckpointConfigMixin, temp_dir: str, checkpoint_dir: Optional[str]) -> str:
    if config.use_checkpoints or config.checkpoint_output:
        if checkpoint_dir is None:
            raise ValueError('must specify a checkpoint directory to use checkpoints and/or checkpoint output')
        return checkpoint_dir
    return temp_dir


def bool_upper_str(val: bool) -> str:
    return str(val).upper()


@dataclass
class SparseGRMStep(CheckpointConfigMixin, JobConfigMixin):
    """Define how the createSparseGRM.R step is executed in a SAIGE pipeline.

    This class is used as an input to SaigeConfig in order to specify
    how the Sparse GRM step is executed in SAIGE.

    Examples
    --------

    Use 16 cores when running the sparse GRM step:

    >>> sparse_grm_step = SparseGRMStep(cpu=16)

    Set the relatedness cutoff to 0.05:

    >>> sparse_grm_step = SparseGRMStep(relatedness_cutoff=0.05)

    Notes
    -----

    You can also create a subclass of this class and redefine the
    job name, job attributes, and output file root name.

    >>> class CustomSparseGRMStep(SparseGRMStep):
    ...     def name(self):
    ...         return 'my_sparse_grm'
    ...
    ...     def attributes(self):
    ...         return {'my_attribute': '1'}
    ...
    ...     def output_root(self, temp_dir: str, checkpoint_dir: Optional[str]) -> str:
    ...         return f'{temp_dir}/my-custom-path'
    """

    memory_chunk_gib: int = 2
    """Pass-through option for "--memoryChunk". Value is in Gi."""

    num_random_markers_for_sparse_kin: int = 200
    """Pass-through option for "--numRandomMarkerforSparseKin". Number of randomly selected markers to be used to identify
        related samples for sparse GRM."""

    relatedness_cutoff: float = 0.125
    """Pass-through option for "--relatednessCutoff". The threshold to treat two samples as unrelated if IsSparseKin is True."""  # FIXME: what is sparse kin

    is_diag_of_kin_set_as_one: bool = False
    """Pass-through option for "--isDiagofKinSetAsOne". Whether to set the diagnal elements in GRM to be 1."""

    min_maf_for_grm: float = 0.01
    """Pass-through option for "--minMAFforGRM". Minimum MAF of markers used for GRM."""

    max_missing_rate_for_grm: Optional[float] = None
    """Pass-through option for "--maxMissingRateforGRM". Maximum missing rate of markers used for GRM."""

    def name(self) -> str:
        """Name of the sparse GRM job in Hail Batch."""
        return 'sparse-grm'

    def output_root(self, temp_dir: str, checkpoint_dir: Optional[str]) -> str:
        """Destination of output files generated by the sparse GRM job in Hail Batch.

        Parameters
        ----------
        temp_dir:
            Remote temporary URL. Example: gs://my-bucket/tmp/

        checkpoint_dir:
            URL to directory to write checkpointed files to. Example: gs://my-bucket/checkpoints/
        """
        working_dir = get_output_dir(self, temp_dir, checkpoint_dir)
        return f'{working_dir}/sparse-grm'

    def attributes(self) -> Optional[Dict]:
        """Attributes to specify for the sparse GRM job in Hail Batch."""
        return None

    async def _call(self, fs: AsyncFS, b: hb.Batch, input_bfile: PlinkResourceGroup, temp_dir: str, checkpoint_dir: str) -> SaigeSparseGRMResourceGroup:
        output_prefix = self.output_root(temp_dir, checkpoint_dir)
        sparse_grm = await load_saige_sparse_grm_file(fs, b, self, output_prefix)
        if sparse_grm is not None:
            return sparse_grm

        create_sparse_grm_j = b.new_job(name=self.name(), attributes=self.attributes())

        (create_sparse_grm_j.cpu(self.cpu).storage(self.storage).image(self.image).spot(self.spot))

        sparse_grm = new_saige_sparse_grm_file(create_sparse_grm_j, self.relatedness_cutoff, self.num_random_markers_for_sparse_kin)

        optional_options = []

        if self.max_missing_rate_for_grm is not None:
            optional_options.append(f'--maxMissingRateforGRM={self.max_missing_rate_for_grm}')

        optional_options_str = '    \\\n'.join(optional_options)

        command = f'''
createSparseGRM.R \\
    --plinkFile={input_bfile} \\
    --nThreads={self.cpu} \\
    --memoryChunk={self.memory_chunk_gib} \\
    --outputPrefix={sparse_grm} \\
    --numRandomMarkerforSparseKin={self.num_random_markers_for_sparse_kin} \\
    --relatednessCutoff={self.relatedness_cutoff} \\
    --isDiagofKinSetAsOne={self.is_diag_of_kin_set_as_one} \\
    --minMAFforGRM={self.min_maf_for_grm} \\
    {optional_options_str}
'''

        create_sparse_grm_j.command(command)

        checkpoint_if_requested(sparse_grm, b, self, output_prefix)

        return sparse_grm


@dataclass
class Step1NullGlmmStep(CheckpointConfigMixin, JobConfigMixin):
    inv_normalize: Optional[bool] = False
    """Pass-through argument for "--invNormalize". Only for quantitative. Whether to perform the inverse normalization for the phenotype"""

    tol: Optional[float] = 0.02
    """Pass-through argument for "--tol". Tolerance for fitting the null GLMM to converge."""

    max_iter: Optional[int] = 20
    """Pass-through argument for "--maxiter". Maximum number of iterations used to fit the null GLMM."""

    tol_pcg: Optional[float] = 1E-5
    """Pass-through argument for "--tolPCG". Tolerance for PCG to converge."""

    max_iter_pcg: Optional[int] = 500
    """Pass-through argument for "--maxiterPCG". Maximum number of iterations for PCG."""

    spa_cutoff: Optional[float] = 2
    """Pass-through argument for "--SPAcutoff". Cutoff for the deviation of score test statistics from mean in the unit of sd to perform SPA."""

    num_random_markers_for_variance_ratio: Optional[int] = 30
    """Pass-through argument for "--numRandomMarkerforVarianceRatio". An integer greater than 0. Number of markers to be randomly selected for 
    estimating the variance ratio. The number will be automatically added by 10 until the coefficient of variantion (CV) for the variance ratio 
    estimate is below ratioCVcutoff"""

    skip_model_fitting: Optional[bool] = False
    """Pass-through argument for "--skipModelFitting". Whether to skip model fitting and only to estimate the variance ratio."""  # FIXME: If TRUE, the file outputPrefix.rda is required

    skip_variance_ratio_estimation: Optional[bool] = False
    """Pass-through argument for "--skipVarianceRatioEstimation". Whether to skip model fitting and only to estimate the variance ratio."""  # FIXME: documentation is wrong. If TRUE, the file outputPrefix.rda is required

    memory_chunk_gib: Optional[int] = 2
    """Pass-through argument for "--memoryChunk". Value is in Gi."""

    tau_init: Optional[List[float]] = field(default_factory=lambda: [0, 0])
    """Pass-through argument for "--tauInit". Initial values for tau."""

    loco: Optional[bool] = True
    """Pass-through argument for "--LOCO". Whether to apply the leave-one-chromosome-out (LOCO) approach when fitting the null model using the full GRM."""

    is_low_mem_loco: Optional[bool] = False
    """Pass-through argument for "--isLowMemLOCO". Whether to output the model file by chromosome when "--LOCO=TRUE". If True, the memory usage will be lower."""

    trace_cv_cutoff: Optional[float] = 0.0025
    """Pass-through argument for "--traceCVcutoff". Threshold for coefficient of variation (CV) for the trace estimator. Number of runs for trace estimation will be increased until the CV is below the threshold."""

    n_run: Optional[int] = 30
    """Pass-through argument for "--nrun". Number of runs in trace estimation."""

    ratio_cv_cutoff: Optional[float] = 0.001
    """Pass-through argument for "--ratioCVcutoff". Threshold for coefficient of variation (CV) for estimating the variance ratio. The number of randomly selected markers will be increased until the CV is below the threshold"""

    is_cate_variance_ratio: Optional[bool] = False
    """Pass-through argument for "--isCateVarianceRatio". Whether to estimate variance ratio based on different MAC categories. If yes, variance ratio will be estimated for multiple MAC categories corresponding to cateVarRatioMinMACVecExclude and cateVarRatioMaxMACVecInclude. Currently, if isCateVarianceRatio=TRUE, then LOCO=FALSE"""

    relatedness_cutoff: Optional[float] = 0
    """Pass-through argument for "--relatednessCutoff". Threshold (minimum relatedness coefficient) to treat two samples as unrelated when the sparse GRM is used."""

    cate_var_ratio_min_mac_vec_exclude: Optional[List[float]] = field(default_factory=lambda: [10, 20.5])
    """Pass-through argument for "--cateVarRatioMinMACVecExclude". Lower bound for MAC categories. The length equals to the number of MAC categories for variance ratio estimation."""

    cate_var_ratio_min_mac_vec_include: Optional[List[float]] = field(default_factory=lambda: [20.5])
    """Pass-through argument for "--cateVarRatioMinMACVecInclude". Higher bound for MAC categories. The length equals to the number of MAC categories for variance ratio estimation minus 1."""

    is_covariate_transform: Optional[bool] = True
    """Pass-through argument for "--isCovariateTransform". Whether use qr transformation on covariates."""

    is_diag_of_kin_set_as_one: Optional[bool] = False
    """Pass-through argument for "--isDiagofKinSetAsOne". Whether to set the diagnal elements in GRM to be 1."""

    use_sparse_grm_to_fit_null: Optional[bool] = False
    """Pass-through argument for "--useSparseGRMtoFitNULL". Whether to use sparse GRM to fit the null model."""

    use_sparse_grm_for_var_ratio: Optional[bool] = False
    """Pass-through argument for "--useSparseGRMforVarRatio". Whether to use sparse GRM to estimate the variance Ratios. If TRUE, the variance ratios will be estimated using the full GRM (numerator) and the sparse GRM (denominator)."""

    min_maf_for_grm: Optional[float] = None
    """Pass-through argument for "--minMAFforGRM". Minimum MAF of markers used for GRM."""

    max_missing_rate_for_grm: Optional[float] = None
    """Pass-through argument for "--maxMissingRateforGRM". Maximum missing rate of markers used for GRM."""

    min_covariate_count: Optional[int] = None
    """Pass-through argument for "--minCovariateCount". Binary covariates with a count less than minCovariateCount will be excluded from the model to avoid convergence issues."""

    include_non_autosomal_markers_for_var_ratio: Optional[bool] = False
    """Pass-through argument for "--includeNonautoMarkersforVarRatio". Whether to allow for non-autosomal markers for variance ratio."""

    female_only: Optional[bool] = None
    """Pass-through argument for "--FemaleOnly". Whether to run null model for females only"""

    male_only: Optional[bool] = None
    """Pass-through argument for "--MaleOnly". Whether to run null model for males only."""

    is_covariate_offset: Optional[bool] = True
    """Pass-through argument for "--isCovariateOffset". Whether to estimate fixed effect coefficients."""

    def output_root(self, output_dir: str, phenotype: Phenotype) -> str:
        """Prefix of output files generated by the null model job in Hail Batch.

        Parameters
        ----------
        output_dir:
            Output directory to write to. Example: gs://my-bucket/checkpoints/
        """
        return f'{output_dir}/null-model-{phenotype.name}'

    def name(self, phenotype: Phenotype) -> str:
        """Name of the null model job in Hail Batch.

        Parameters
        ----------
        phenotype:
            The phenotype being analyzed in this SAIGE job.
        """
        return f'null-model-{phenotype.name}'

    def attributes(self, analysis_type: SaigeAnalysisType, phenotype: Phenotype) -> Optional[Dict]:
        """Attributes to specify for the null model job in Hail Batch.

        Parameters
        ----------
        analysis_type:
            Analysis type for this null model job.
        phenotype:
            The phenotype being analyzed in this null model SAIGE job.
        """
        return {'analysis_type': analysis_type.value, 'trait_type': phenotype.phenotype_type.value}

    async def _call(
        self,
        fs: AsyncFS,
        b: hb.Batch,
        *,
        input_bfile: PlinkResourceGroup,
        input_phenotypes: TextResourceFile,
        keep_samples_list: Optional[TextResourceFile],
        phenotype: Phenotype,
        analysis_type: SaigeAnalysisType,
        phenotype_config: PhenotypeConfig,
        sparse_grm: Optional[SaigeSparseGRMResourceGroup],
        temp_dir: str,
        checkpoint_dir: Optional[str],
    ) -> SaigeGLMMResourceGroup:
        working_dir = get_output_dir(self, temp_dir, checkpoint_dir)
        output_root = self.output_root(working_dir, phenotype)

        glmm_resource_output = await load_saige_glmm_file(fs, b, self, output_root, analysis_type)
        if glmm_resource_output:
            return glmm_resource_output

        j = (
            b.new_job(
                name=self.name(phenotype=phenotype),
                attributes=self.attributes(analysis_type, phenotype),
            )
            .storage(self.storage)
            .image(self.image)
            .cpu(self.cpu)
            .memory(self.memory)
            .spot(self.spot)
        )

        null_glmm = new_saige_glmm_file(j, analysis_type)

        command = self._command(
            input_bfile,
            input_phenotypes,
            keep_samples_list,
            phenotype_config,
            phenotype,
            null_glmm,
            sparse_grm,
        )

        j.command(command)

        checkpoint_if_requested(null_glmm, b, self, output_root)

        return null_glmm

    def _command(
        self,
        input_bfile: PlinkResourceGroup,
        phenotypes_file: TextResourceFile,
        keep_samples_list: Optional[TextResourceFile],
        phenotype_config: PhenotypeConfig,
        phenotype: Phenotype,
        null_glmm: SaigeGLMMResourceGroup,
        sparse_grm: Optional[SaigeSparseGRMResourceGroup],
    ) -> str:
        test_type = saige_phenotype_to_test_type[phenotype.phenotype_type]

        covariates = [cov.name for cov in phenotype_config.covariates]

        options = [
            f'--plinkFile={input_bfile}',
            f'--phenoFile={phenotypes_file}',
            f'--covarColList={",".join(covariates)}',
            f'--phenoCol={phenotype.name}',
            f'--sampleIDColinphenoFile={phenotype_config.sample_id_col}',
            f'--traitType={test_type.value}',
            f'--outputPrefix={null_glmm}',
            f'--outputPrefix_varRatio={null_glmm}',
            f'--nThreads={self.cpu}',
        ]

        if sparse_grm is not None:
            options.append(f'--sparseGRMFile={sparse_grm.grm}')
            options.append(f'--sparseGRMSampleIDFile={sparse_grm.sample_ids}')

        # FIXME: is this really binary or should it be quantitative?
        q_covariates = [cov.name for cov in phenotype_config.covariates if saige_phenotype_to_test_type[cov.phenotype_type] == SaigeTestType.BINARY]
        if len(q_covariates) > 0:
            options.append(f'--qCovarColList={",".join(q_covariates)}')

        if keep_samples_list is not None:
            options.append(f'--SampleIDIncludeFile={keep_samples_list}')
        if self.inv_normalize is not None:
            options.append(f'--invNormalize={bool_upper_str(self.inv_normalize)}')
        if self.tol is not None:
            options.append(f'--tol={self.tol}')
        if self.max_iter is not None:
            options.append(f'--maxiter={self.max_iter}')
        if self.tol_pcg is not None:
            options.append(f'--tolPCG={self.tol_pcg}')
        if self.max_iter_pcg is not None:
            options.append(f'--maxiterPCG={self.max_iter_pcg}')
        if self.spa_cutoff is not None:
            options.append(f'--SPAcutoff={self.spa_cutoff}')
        if self.num_random_markers_for_variance_ratio is not None:
            options.append(f'--numRandomMarkerforVarianceRatio={self.num_random_markers_for_variance_ratio}')
        if self.skip_model_fitting is not None:
            options.append(f'--skipModelFitting={bool_upper_str(self.skip_model_fitting)}')
        if self.skip_variance_ratio_estimation is not None:
            options.append(f'--skipVarianceRatioEstimation={bool_upper_str(self.skip_variance_ratio_estimation)}')
        if self.memory_chunk_gib is not None:
            options.append(f'--memoryChunk={self.memory_chunk_gib}')
        if self.tau_init is not None:
            tau_init = ','.join(str(tau) for tau in self.tau_init)
            options.append(f'--tauInit={tau_init}')
        if self.loco is not None:
            options.append(f'--LOCO={bool_upper_str(self.loco)}')
        if self.is_low_mem_loco is not None:
            options.append(f'--isLowMemLOCO={bool_upper_str(self.is_low_mem_loco)}')
        if self.trace_cv_cutoff is not None:
            options.append(f'--traceCVcutoff={self.trace_cv_cutoff}')
        if self.n_run is not None:
            options.append(f'--nrun={self.n_run}')
        if self.ratio_cv_cutoff is not None:
            options.append(f'--ratioCVcutoff={self.ratio_cv_cutoff}')
        if self.is_cate_variance_ratio is not None:
            options.append(f'--isCateVarianceRatio={bool_upper_str(self.is_cate_variance_ratio)}')
        if self.relatedness_cutoff is not None:
            options.append(f'--relatednessCutoff={self.relatedness_cutoff}')
        if self.cate_var_ratio_min_mac_vec_exclude is not None:
            exclude_str = ','.join(str(item) for item in self.cate_var_ratio_min_mac_vec_exclude)
            options.append(f'--cateVarRatioMinMACVecExclude={exclude_str}')
        if self.cate_var_ratio_min_mac_vec_include is not None:
            include_str = ','.join(str(item) for item in self.cate_var_ratio_min_mac_vec_include)
            options.append(f'--cateVarRatioMinMACVecInclude={include_str}')
        if self.is_covariate_transform is not None:
            options.append(f'--isCovariateTransform={bool_upper_str(self.is_covariate_transform)}')
        if self.is_diag_of_kin_set_as_one is not None:
            options.append(f'--isDiagofKinSetAsOne={bool_upper_str(self.is_diag_of_kin_set_as_one)}')
        if self.use_sparse_grm_to_fit_null is not None:
            options.append(f'--useSparseGRMtoFitNULL={bool_upper_str(self.use_sparse_grm_to_fit_null)}')
        if self.use_sparse_grm_for_var_ratio is not None:
            options.append(f'--useSparseGRMforVarRatio={bool_upper_str(self.use_sparse_grm_for_var_ratio)}')
        if self.min_maf_for_grm is not None:
            options.append(f'--minMAFforGRM={self.min_maf_for_grm}')
        if self.max_missing_rate_for_grm is not None:
            options.append(f'--maxMissingRateforGRM={self.max_missing_rate_for_grm}')
        if self.min_covariate_count is not None:
            options.append(f'--minCovariateCount={self.min_covariate_count}')
        if self.include_non_autosomal_markers_for_var_ratio is not None:
            options.append(f'--includeNonautoMarkersforVarRatio={bool_upper_str(self.include_non_autosomal_markers_for_var_ratio)}')
        if self.female_only is not None:
            if phenotype_config.sex_col is None:
                raise ValueError('Cannot specify female only. No sex column specified.')
            if phenotype_config.female_code is None:
                raise ValueError('Cannot specify female only. No code specified for females.')
            options.append(f'--FemaleOnly={bool_upper_str(self.female_only)}')
            options.append(f'--sexCol={phenotype_config.sex_col}')
            options.append(f'--FemaleCode={phenotype_config.female_code}')
        if self.male_only is not None:
            if phenotype_config.sex_col is None:
                raise ValueError('Cannot specify male only. No sex column specified.')
            if phenotype_config.male_code is None:
                raise ValueError('Cannot specify male only. No code specified for males.')
            options.append(f'--MaleOnly={bool_upper_str(self.male_only)}')
            options.append(f'--sexCol={phenotype_config.sex_col}')
            options.append(f'--MaleCode={phenotype_config.male_code}')
        if self.is_covariate_offset is not None:
            options.append(f'--isCovariateOffset={bool_upper_str(self.is_covariate_offset)}')

        options_str = '  \\\n'.join(options)

        command = f'''
set -o pipefail;

perl -pi -e s/^chr// {input_bfile.bim};

step1_fitNULLGLMM.R \\
    {options_str}
'''
        return command


@dataclass
class Step2SPAStep(CheckpointConfigMixin, JobConfigMixin):
    save_stdout: bool = True
    mkl_off: bool = False
    drop_missing_dosages: bool = True  # FIXME
    min_mac: float = 0.5  # FIXME
    min_maf: float = 0
    max_maf_for_group_test: Optional[List[float]] = None
    min_info: float = 0
    spa_cutoff: float = 2.0
    output_af_in_case_control: bool = False
    output_n_in_case_control: bool = False
    output_het_hom_counts: bool = False
    kernel: Optional[str] = None  # FIXME
    method: Optional[str] = None  # FIXME
    weights_beta_rare: Optional[float] = None  # FIXME
    weights_beta_common: Optional[float] = None  # FIXME
    weight_maf_cutoff: Optional[float] = None  # FIXME
    r_corr: Optional[float] = None  # FIXME
    single_variant_in_group_test: bool = False
    output_maf_in_case_control_in_group_test: bool = False
    cate_var_ratio_min_mac_vec_exclude: Optional[List[float]] = None
    cate_var_ratio_max_mac_vec_include: Optional[List[float]] = None
    dosage_zerod_cutoff: float = 0.2
    output_pvalue_na_in_group_test_for_binary: bool = False
    account_for_case_control_imbalance_in_group_test: bool = True
    weights_include_in_group_file: bool = False  # fixme with weight
    weights_for_g2_cond: Optional[List[int]] = None
    output_beta_se_in_burden_test: bool = False
    output_logp_for_single: bool = False
    x_par_region: Optional[List[str]] = None
    rewrite_x_nonpar_for_males: bool = False
    method_to_collapse_ultra_rare: str = 'absence_or_presence'
    mac_cutoff_to_collapse_ultra_rare: float = 10
    dosage_cutoff_for_ultra_rare_presence: float = 0.5
    loco: bool = False

    def name(self, phenotype: Phenotype, chunk: VariantChunk) -> str:
        return f'step2-spa-{phenotype.name}-{chunk.idx}'

    def attributes(
        self, *, analysis_type: SaigeAnalysisType, phenotype: Phenotype, chunk: VariantChunk
    ) -> Optional[Dict]:
        return {
            'analysis_type': analysis_type.value,
            'trait_type': phenotype.phenotype_type.value,
            'phenotype': phenotype.name,
            'chunk': chunk.name,
        }

    def log_file_prefix(
        self, temp_dir: str, checkpoint_dir: Optional[str], phenotype_name: str, chunk: VariantChunk
    ) -> str:
        working_dir = get_output_dir(self, temp_dir, checkpoint_dir)
        return f'{working_dir}/logs/{phenotype_name}/{chunk.idx}'

    def output_file_prefix(
        self, temp_dir: str, checkpoint_dir: Optional[str], phenotype_name: str, chunk: VariantChunk
    ) -> str:
        working_dir = get_output_dir(self, temp_dir, checkpoint_dir)
        return f'{working_dir}/results/{phenotype_name}/{chunk.idx}'

    def output_glob(self, temp_dir: str, checkpoint_dir: Optional[str], phenotype_name: str) -> str:
        working_dir = get_output_dir(self, temp_dir, checkpoint_dir)
        return f'{working_dir}/results/{phenotype_name}/*'

    def command(
        self,
        *,
        mt_path: str,
        analysis_type: SaigeAnalysisType,
        null_model: Union[SaigeGeneGLMMResourceGroup, SaigeGLMMResourceGroup],
        input_data_type: SaigeInputDataType,
        chunk: VariantChunk,
        phenotype: Phenotype,
        result: SaigeResultResourceGroup,
        stdout: TextResourceFile,
        sparse_grm: Optional[SaigeSparseGRMResourceGroup],
        group_annotations: Optional[TextResourceFile],
    ):
        if self.mkl_off:
            mkl_off = 'export MKL_NUM_THREADS=1; export MKL_DYNAMIC=false; export OMP_NUM_THREADS=1; export OMP_DYNAMIC=false; '
        else:
            mkl_off = ''

        if input_data_type == SaigeInputDataType.VCF:
            export_cmd = f'hl.export_vcf(mt.select_entries("GT"), "/data.vcf.bgz")'
            input_flags = [
                f'--vcfFile=/data.vcf.bgz',
                f'--vcfFileIndex=/data.vcf.bgz.csi',
                f'--vcfField=GT',
            ]
            index_cmd = 'tabix -C /data.vcf.bgz'
        else:
            export_cmd = f'hl.export_bgen(mt, "/data")'
            input_flags = [
                f'--bgenFile=/data.bgen',
                f'--bgenFileIndex=/data.bgen.idx',
                f'--sampleFile=/data.sample',
            ]
            index_cmd = ''

        if analysis_type == SaigeAnalysisType.GENE:
            assert sparse_grm is not None and group_annotations is not None
            assert chunk.groups is not None
            group_ann_filter_cmd = f'''
cat > filter_gene_annotations.py <<EOF
import hail as hl
import json
annotations = hl.import_table("{group_annotations}")
groups = json.loads("{json.dumps(chunk.groups)}")
annotations = annotations.filter(hl.is_defined(groups[annotations.group]))  # fixme: what is the right field name here?
EOF
'''
        else:
            group_ann_filter_cmd = ''

        hail_io_cmd = f'''
cat > read_from_mt.py <<EOF
import hail as hl
mt = hl.read_matrix_table("{mt_path}")
interval = hl.parse_locus_interval("{chunk.to_interval_str()}", reference_genome=mt.locus.dtype.reference_genome)
mt = mt.filter_rows(interval.contains(mt.locus))
{export_cmd}
EOF
python3 read_from_mt.py
{index_cmd}
'''

        saige_options = [
            f'--chrom={chunk.interval.start.contig}',
            f'--minMAF={self.min_maf}',
            f'--minMAC={self.min_mac}',
            f'--GMMATmodelFile={null_model.rda}',
            f'--varianceRatioFile={null_model.variance_ratio}',
            f'--SAIGEOutputFile={result}',
            f'--SPAcutoff={self.spa_cutoff}',
            f'--LOCO={str(self.loco).upper()}',
            # f'--IsOutputAFinCaseCtrl={str(self.output_af_in_case_control).upper()}',
            # f'--IsOutputNinCaseCtrl={str(self.output_n_in_case_control).upper()}',
            # f'--IsOutputHetHomCountsinCaseCtrl={str(self.output_het_hom_counts).upper()}',
            # f'--IsOutputAFinCaseCtrl={str(self.output_af_in_case_control).upper()}',
            # f'--IsOutputMAFinCaseCtrlinGroupTest={str(self.output_maf_in_case_control_in_group_test).upper()}',
            # f'--IsOutputlogPforSingle={str(self.output_logp_for_single).upper()}',
        ]

        # if self.max_maf_for_group_test is not None:
        #     saige_options.append(f'maxMAF_in_groupTest={",".join(str(maf) for maf in self.max_maf_for_group_test)}')
        # if self.kernel is not None:
        #     saige_options.append(f'--kernel={self.kernel}')
        # if self.method is not None:
        #     saige_options.append(f'--method={self.method}')
        # if self.weights_beta_rare is not None:
        #     saige_options.append(f'--weights.beta.rare={self.weights_beta_rare}')
        # if self.weights_beta_common is not None:
        #     saige_options.append(f'--weights.beta.common={self.weights_beta_common}')
        # if self.weight_maf_cutoff is not None:
        #     saige_options.append(f'--weightMAFcutoff={self.weight_maf_cutoff}')
        # if self.r_corr is not None:
        #     saige_options.append(f'--r.corr={self.r_corr}')
        # if self.cate_var_ratio_min_mac_vec_exclude is not None:
        #     exclude = ','.join(str(val) for val in self.cate_var_ratio_min_mac_vec_exclude)
        #     saige_options.append(f'--cateVarRatioMinMACVecExclude={exclude}')
        # if self.cate_var_ratio_max_mac_vec_include is not None:
        #     include = ','.join(str(val) for val in self.cate_var_ratio_max_mac_vec_include)
        #     saige_options.append(f'--cateVarRatioMaxMACVecInclude={include}')
        # if self.dosage_zerod_cutoff is not None:
        #     saige_options.append(f'--dosageZerodCutoff={self.dosage_zerod_cutoff}')
        # if self.output_pvalue_na_in_group_test_for_binary is not None:
        #     saige_options.append(
        #         f'--IsOutputPvalueNAinGroupTestforBinary={str(self.output_pvalue_na_in_group_test_for_binary).upper()}'
        #     )
        # if self.account_for_case_control_imbalance_in_group_test is not None:
        #     saige_options.append(
        #         f'--IsAccountforCasecontrolImbalanceinGroupTest={str(self.account_for_case_control_imbalance_in_group_test).upper()}'
        #     )
        # if self.x_par_region is not None:
        #     regions = ','.join(self.x_par_region)
        #     saige_options.append(f'--X_PARregion={regions}')
        # if self.rewrite_x_nonpar_for_males is not None:
        #     saige_options.append(f'--is_rewrite_XnonPAR_forMales={str(self.rewrite_x_nonpar_for_males).upper()}')
        # if self.method_to_collapse_ultra_rare is not None:
        #     saige_options.append(f'--method_to_CollapseUltraRare={self.method_to_collapse_ultra_rare}')
        # if self.mac_cutoff_to_collapse_ultra_rare is not None:
        #     saige_options.append(f'--MACCutoff_to_CollapseUltraRare={self.mac_cutoff_to_collapse_ultra_rare}')
        # if self.dosage_cutoff_for_ultra_rare_presence is not None:
        #     saige_options.append(f'--DosageCutoff_for_UltraRarePresence={self.dosage_cutoff_for_ultra_rare_presence}')

        # FIXME: --weightsIncludeinGroupFile, --weights_for_G2_cond, --sampleFile_male

        gene_options = []

        if analysis_type == SaigeAnalysisType.GENE:
            test_type = saige_phenotype_to_test_type[phenotype.phenotype_type]
            if test_type == SaigeTestType.BINARY:
                gene_options = [
                    f'--IsOutputPvalueNAinGroupTestforBinary={str(self.output_pvalue_na_in_group_test_for_binary).upper()}'
                ]
            else:
                gene_options = [
                    f'--groupFile={group_annotations}',
                    f'--sparseSigmaFile={sparse_grm.grm}',
                    f'--IsSingleVarinGroupTest={str(self.single_variant_in_group_test).upper()}',
                    f'--IsOutputBETASEinBurdenTest={str(self.output_beta_se_in_burden_test).upper()}',
                ]

        input_flags = '    \\\n'.join(input_flags)
        saige_options = '    \\\n'.join(saige_options)
        gene_options = '    \\\n'.join(gene_options)

        command = f'''
set -o pipefail;
{mkl_off}

{hail_io_cmd}
{group_ann_filter_cmd}

step2_SPAtests.R \\
{input_flags} \\
{saige_options} \\
{gene_options} \\
    2>&1 | tee {stdout};
'''

        return command

    async def _call(
        self,
        fs: AsyncFS,
        b: hb.Batch,
        *,
        mt_path: str,
        temp_dir: str,
        checkpoint_dir: Optional[str],
        analysis_type: SaigeAnalysisType,
        null_model: Union[SaigeGeneGLMMResourceGroup, SaigeGLMMResourceGroup],
        input_data_type: SaigeInputDataType,
        chunk: VariantChunk,
        phenotype: Phenotype,
        sparse_grm: Optional[SaigeSparseGRMResourceGroup] = None,
        group_annotations: Optional[TextResourceFile] = None,
    ) -> Union[SaigeGeneResultResourceGroup, SaigeResultResourceGroup]:
        output_root = self.output_file_prefix(temp_dir, checkpoint_dir, phenotype.name, chunk)
        log_root = self.log_file_prefix(temp_dir, checkpoint_dir, phenotype.name, chunk)

        results = await load_saige_result_file(fs, b, self, output_root, analysis_type)
        if results is not None:
            return results

        j = (
            b.new_job(
                name=self.name(phenotype=phenotype, chunk=chunk),
                attributes=self.attributes(
                    phenotype=phenotype,
                    chunk=chunk,
                    analysis_type=analysis_type,
                ),
            )
            .storage(self.storage)
            .image(self.image)
            .cpu(self.cpu)
            .memory(self.memory)
            .spot(self.spot)
        )

        results = new_saige_result_file(j, analysis_type)

        command = self.command(
            mt_path=mt_path,
            analysis_type=analysis_type,
            null_model=null_model,
            input_data_type=input_data_type,
            chunk=chunk,
            phenotype=phenotype,
            result=results,
            stdout=j.stdout,
            sparse_grm=sparse_grm,
            group_annotations=group_annotations,
        )

        j.command(command)

        b.write_output(results, output_root)

        if self.save_stdout:
            b.write_output(j.stdout, f'{log_root}.log')

        return results


@dataclass
class CompilePhenotypeResultsStep(CheckpointConfigMixin, JobConfigMixin):
    def name(self, phenotype: Phenotype) -> str:
        return f'compile-results-{phenotype.name}'

    def attributes(self, *, phenotype: Phenotype) -> Optional[Dict]:
        return {'phenotype': phenotype.name}

    def results_path_glob(self, temp_dir: str, checkpoint_dir: Optional[str]):
        working_dir = get_output_dir(self, temp_dir, checkpoint_dir)
        return f'{working_dir}/compiled-results/*.tsv'

    def output_file(self, temp_dir: str, checkpoint_dir: Optional[str], phenotype_name: str) -> str:
        working_dir = get_output_dir(self, temp_dir, checkpoint_dir)
        return f'{working_dir}/compiled-results/{phenotype_name}.tsv'  # FIXME: compress this

    async def check_if_output_exists(self,
                                     fs: AsyncFS,
                                     b: hb.Batch,
                                     phenotype: Phenotype,
                                     temp_dir: str,
                                     checkpoint_dir: Optional[str]) -> Optional[TextResourceFile]:
        output_file = self.output_file(temp_dir, checkpoint_dir, phenotype.name)
        return await load_text_file(fs, b, self, output_file)

    def command(self, results_path: str, phenotype_name: str, output_file: str) -> str:
        return f'''
cat > compile_results.py <<EOF
import hail as hl
ht = hl.import_table("{results_path}", impute=True)
ht = ht.annotate(phenotype="{phenotype_name}")
ht.export("{output_file}")
EOF
python3 compile_results.py
'''

    async def _call(
        self,
        fs: AsyncFS,
        b: hb.Batch,
        phenotype: Phenotype,
        results_path: str,
        dependencies: List[hb.Job],
        temp_dir: str,
        checkpoint_dir: Optional[str],
    ) -> TextResourceFile:
        output_file = self.output_file(temp_dir, checkpoint_dir, phenotype.name)
        results = await load_text_file(fs, b, self, output_file)
        if results is not None:
            return results

        j = (b
             .new_job(name=self.name(phenotype), attributes=self.attributes(phenotype=phenotype))
             .image(self.image)
             .cpu(self.cpu)
             .memory(self.memory)
             .depends_on(*dependencies)
        )

        compiled_results = new_text_file(j)

        cmd = self.command(results_path, phenotype.name, compiled_results)

        j.command(cmd)

        b.write_output(compiled_results, output_file)

        return compiled_results


@dataclass
class CompileAllResultsStep(CheckpointConfigMixin, JobConfigMixin):
    overwrite: bool = True

    def name(self) -> str:
        return 'compile-all-results'

    def command(self, mt_path: str, results_path: str, results_ht: HailTableResourceFile) -> str:
        return f'''
cat > compile_results.py <<EOF
import hail as hl
mt = hl.read_matrix_table("{mt_path}")
reference_genome=mt.locus.dtype.reference_genome
ht = hl.import_table("{results_path}", impute=True)
ht = ht.annotate(locus=hl.locus(hl.str(ht.CHR), ht.POS, reference_genome=reference_genome), alleles=hl.array([ht.Allele1, ht.Allele2]))
ht = ht.key_by(ht.locus, ht.alleles, ht.phenotype)
ht = ht.drop('CHR', 'POS', 'Allele1', 'Allele2')
ht.write("{results_ht}")
EOF
python3 compile_results.py
'''

    async def _call(
        self,
        fs: AsyncFS,
        b: hb.Batch,
        results_path: str,
        output_ht_path: str,
        dependencies: List[hb.Job],
        mt_path: str,
    ):
        if fs.isdir(output_ht_path) and not self.overwrite:
            return

        j = (b
             .new_job(name=self.name())
             .image(self.image)
             .cpu(self.cpu)
             .memory(self.memory)
             .depends_on(*dependencies)
        )

        results_ht = new_hail_table(j)

        cmd = self.command(mt_path, results_path, results_ht)

        j.command(cmd)

        b.write_output(results_ht, output_ht_path)

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook explores variability in hail's python (macro)-benchmarks when  \n",
    "said benchmarks are executed on the hail batch service. The analyses within  \n",
    "are based off the methods proposed in [1], albeit slightly modified for long  \n",
    "running benchmarks. The goals of these analyses are  \n",
    "\n",
    "- to determine if we can detect slowdowns of 5% or less reliably when running  \n",
    "  benchmarks on hail batch.  \n",
    "- to identify configurations (number of batch jobs x iterations) that allow us  \n",
    "  to detect slowdowns efficiently (ie without excesssive time and money).  \n",
    "\n",
    "[1] Laaber et al., Software Microbenchmarking in the Cloud. How Bad is it Really?  \n",
    "    https://dl.acm.org/doi/10.1007/s10664-019-09681-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import plotly.io as pio\n",
    "import yaml\n",
    "from benchmark.tools import annotate_index, maybe\n",
    "from benchmark.tools.impex import import_benchmarks\n",
    "from benchmark.tools.plotting import (\n",
    "    plot_iteration_against_time,\n",
    "    plot_mean_time_per_instance,\n",
    ")\n",
    "from benchmark.tools.statistics import (\n",
    "    laaber_mds,\n",
    "    schultz_mds,\n",
    "    variability,\n",
    ")\n",
    "from IPython.display import Pretty, clear_output, display\n",
    "from plotly.offline import init_notebook_mode\n",
    "\n",
    "import hail as hl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = str(Path().absolute())\n",
    "hl.init(backend='spark', quiet=True)\n",
    "\n",
    "init_notebook_mode()\n",
    "pio.renderers.default = 'notebook_connected'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import benchmark data\n",
    "\n",
    "Benchmarks under `hail/python/benchmarks` are executed with a custom pytest  \n",
    "plugin and their results are output as json lines (.jsonl). Unscrupulously,  \n",
    "we use hail to analyse itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with hl.TemporaryDirectory() as tmpdir:\n",
    "    ht = import_benchmarks(Path(f'{prefix}/in/benchmarks.jsonl'), tmpdir=tmpdir)\n",
    "    ht = ht.checkpoint(f\"{prefix}/out/benchmarks.ht\")\n",
    "\n",
    "benchmarks = ht.aggregate(hl.agg.collect_as_set(ht.path + hl.str('::') + ht.name))\n",
    "benchmarks = sorted(benchmarks)\n",
    "print(*benchmarks, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In these next sections, we'll estimate the number of iterations required for  \n",
    "a benchmark to reach a steady-state - the so-called \"burn-in\" iterations. It  \n",
    "would be nice to automate this. Laaber et al. reference some techniques but  \n",
    "that's beyond the scope of this work for now. \n",
    "\n",
    "The process of estimating the number of burn-in iterations is subjective and   \n",
    "requires some amount of eye-balling; by plotting iteration vs execution time  \n",
    "for all instances, you select an iteration number after which execution time  \n",
    "doesn't decay. Easier said than done because not all benchmarks read a steady-  \n",
    "state. As always, use your best judgement.  \n",
    "\n",
    "The following cell contains some values that I eye-balled. It's included as a  \n",
    "separate cell in case you want to take them as granted and skip ahead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_stable_index = {\n",
    "    'test_analyze_benchmarks': 5,\n",
    "    'test_block_matrix_to_matrix_table_row_major': 4,\n",
    "    'test_blockmatrix_write_from_entry_expr_range_mt_standardize': 8,\n",
    "    'test_blockmatrix_write_from_entry_expr_range_mt': 10,\n",
    "    'test_concordance': 2,\n",
    "    'test_export_range_matrix_table_col_p100': 15,\n",
    "    'test_export_range_matrix_table_entry_field_p100': 3,\n",
    "    'test_export_range_matrix_table_row_p100': 8,\n",
    "    'test_export_vcf': 8,\n",
    "    'test_genetics_pipeline': 4,\n",
    "    'test_gnomad_coverage_stats_optimized': 5,\n",
    "    'test_gnomad_coverage_stats': 5,\n",
    "    'test_group_by_collect_per_row': 5,\n",
    "    'test_group_by_take_rekey': 10,\n",
    "    'test_hwe_normalized_pca_blanczos_small_data_0_iterations': 8,\n",
    "    'test_hwe_normalized_pca_blanczos_small_data_10_iterations': 8,\n",
    "    'test_hwe_normalized_pca': 6,\n",
    "    'test_import_and_transform_gvcf': 2,\n",
    "    'test_import_bgen_filter_count': 18,\n",
    "    'test_import_bgen_force_count_all': 4,\n",
    "    'test_import_bgen_force_count_just_gp': 20,\n",
    "    'test_import_bgen_info_score': 12,\n",
    "    'test_import_gvcf_force_count': 2,\n",
    "    'test_import_vcf_count_rows': 1,\n",
    "    'test_import_vcf_write': 5,\n",
    "    'test_join_partitions_table[10-10]': 4,\n",
    "    'test_join_partitions_table[10-100]': 2,\n",
    "    'test_join_partitions_table[10-1000]': 5,\n",
    "    'test_join_partitions_table[100-10]': 10,\n",
    "    'test_join_partitions_table[100-100]': 10,\n",
    "    'test_join_partitions_table[100-1000]': 8,\n",
    "    'test_join_partitions_table[1000-10]': 12,\n",
    "    'test_join_partitions_table[1000-100]': 10,\n",
    "    'test_join_partitions_table[1000-1000]': 8,\n",
    "    'test_kyle_sex_specific_qc': 6,\n",
    "    'test_large_range_matrix_table_sum': 5,\n",
    "    'test_ld_prune_profile_25': 10,\n",
    "    'test_linear_regression_rows': 10,\n",
    "    'test_logistic_regression_rows_wald': 5,\n",
    "    'test_make_ndarray': 5,\n",
    "    'test_matrix_table_aggregate_entries': 8,\n",
    "    'test_matrix_table_array_arithmetic': 20,\n",
    "    'test_matrix_table_call_stats_star_star': 8,\n",
    "    'test_matrix_table_cols_show': 5,\n",
    "    'test_matrix_table_decode_and_count_just_gt': 5,\n",
    "    'test_matrix_table_decode_and_count': 8,\n",
    "    'test_matrix_table_entries_show': 4,\n",
    "    'test_matrix_table_entries_table_no_key': 4,\n",
    "    'test_matrix_table_entries_table': 10,\n",
    "    'test_matrix_table_filter_entries_unfilter': 8,\n",
    "    'test_matrix_table_filter_entries': 6,\n",
    "    'test_matrix_table_many_aggs_col_wise': 3,\n",
    "    'test_matrix_table_many_aggs_row_wise': 2,\n",
    "    'test_matrix_table_nested_annotate_rows_annotate_entries': 4,\n",
    "    'test_matrix_table_rows_force_count': 20,\n",
    "    'test_matrix_table_rows_is_transition': 5,\n",
    "    'test_matrix_table_rows_show': 10,\n",
    "    'test_matrix_table_scan_count_cols_2': 20,\n",
    "    'test_matrix_table_scan_count_rows_2': 5,\n",
    "    'test_matrix_table_show': 7,\n",
    "    'test_matrix_table_take_col': 10,\n",
    "    'test_matrix_table_take_entry': 8,\n",
    "    'test_matrix_table_take_row': 10,\n",
    "    'test_minimal_detectable_slowdown[laaber_mds]': 5,\n",
    "    'test_minimal_detectable_slowdown[schultz_mds]': 6,\n",
    "    'test_mt_group_by_memory_usage': 5,\n",
    "    'test_mt_localize_and_collect': 5,\n",
    "    'test_ndarray_addition': 10,\n",
    "    'test_ndarray_matmul_float64': 6,\n",
    "    'test_ndarray_matmul_int64': 10,\n",
    "    'test_pc_relate_5k_5k': 4,\n",
    "    'test_pc_relate': 3,\n",
    "    'test_per_row_stats_star_star': 10,\n",
    "    'test_python_only_10k_combine': 6,\n",
    "    'test_python_only_10k_transform': 10,\n",
    "    'test_read_decode_gnomad_coverage': 10,\n",
    "    'test_read_force_count_partitions[10]': 10,\n",
    "    'test_read_force_count_partitions[100]': 8,\n",
    "    'test_read_force_count_partitions[1000]': 10,\n",
    "    'test_read_with_index[1000]': 8,\n",
    "    'test_sample_qc': 3,\n",
    "    'test_sentinel_cpu_hash_1': 5,\n",
    "    'test_sentinel_read_gunzip': 10,\n",
    "    'test_shuffle_key_by_aggregate_bad_locality': 8,\n",
    "    'test_shuffle_key_by_aggregate_good_locality': 5,\n",
    "    'test_shuffle_key_rows_by_4096_byte_rows': 2,\n",
    "    'test_shuffle_key_rows_by_65k_byte_rows': 4,\n",
    "    'test_shuffle_key_rows_by_mt': 10,\n",
    "    'test_shuffle_order_by_10m_int': 10,\n",
    "    'test_split_multi_hts': 10,\n",
    "    'test_split_multi': 8,\n",
    "    'test_sum_table_of_ndarrays': 5,\n",
    "    'test_table_aggregate_approx_cdf': 4,\n",
    "    'test_table_aggregate_array_sum': 5,\n",
    "    'test_table_aggregate_counter': 10,\n",
    "    'test_table_aggregate_downsample_dense': 5,\n",
    "    'test_table_aggregate_downsample_worst_case': 10,\n",
    "    'test_table_aggregate_int_stats': 5,\n",
    "    'test_table_aggregate_linreg': 8,\n",
    "    'test_table_aggregate_take_by_strings': 10,\n",
    "    'test_table_annotate_many_flat': 5,\n",
    "    'test_table_big_aggregate_compilation': 4,\n",
    "    'test_table_big_aggregate_compile_and_execute': 2,\n",
    "    'test_table_expr_take': 25,\n",
    "    'test_table_foreign_key_join[1000000-1000]': 3,\n",
    "    'test_table_foreign_key_join[1000000-1000000]': 3,\n",
    "    'test_table_group_by_aggregate_sorted': 8,\n",
    "    'test_table_group_by_aggregate_unsorted': 7,\n",
    "    'test_table_import_ints_impute': 8,\n",
    "    'test_table_import_ints': 4,\n",
    "    'test_table_import_strings': 3,\n",
    "    'test_table_key_by_shuffle': 3,\n",
    "    'test_table_python_construction': 10,\n",
    "    'test_table_range_array_range_force_count': 6,\n",
    "    'test_table_range_force_count': 10,\n",
    "    'test_table_range_join[1000000000-1000]': 20,\n",
    "    'test_table_range_join[1000000000-1000000000]': 10,\n",
    "    'test_table_range_means': 10,\n",
    "    'test_table_read_force_count_ints': 5,\n",
    "    'test_table_read_force_count_strings': 4,\n",
    "    'test_table_scan_prev_non_null': 20,\n",
    "    'test_table_scan_sum_1k_partitions': 2,\n",
    "    'test_table_show': 12,\n",
    "    'test_table_take': 20,\n",
    "    'test_test_head_and_tail_region_memory': 10,\n",
    "    'test_test_inner_join_region_memory': 10,\n",
    "    'test_test_left_join_region_memory': 10,\n",
    "    'test_test_map_filter_region_memory': 10,\n",
    "    'test_union_partitions_table[10-10]': 4,\n",
    "    'test_union_partitions_table[10-100]': 5,\n",
    "    'test_union_partitions_table[10-1000]': 8,\n",
    "    'test_union_partitions_table[100-10]': 16,\n",
    "    'test_union_partitions_table[100-100]': 5,\n",
    "    'test_union_partitions_table[100-1000]': 15,\n",
    "    'test_union_partitions_table[1000-10]': 10,\n",
    "    'test_union_partitions_table[1000-100]': 6,\n",
    "    'test_union_partitions_table[1000-1000]': 15,\n",
    "    'test_variant_and_sample_qc_nested_with_filters_2': 2,\n",
    "    'test_variant_and_sample_qc_nested_with_filters_4_counts': 8,\n",
    "    'test_variant_and_sample_qc_nested_with_filters_4': 10,\n",
    "    'test_variant_and_sample_qc': 10,\n",
    "    'test_variant_qc': 5,\n",
    "    'test_vds_combiner_chr22': 10,\n",
    "    'test_write_profile_mt': 8,\n",
    "    'test_write_range_matrix_table_p100': 2,\n",
    "    'test_write_range_table[10000000-10]': 3,\n",
    "    'test_write_range_table[10000000-100]': 3,\n",
    "    'test_write_range_table[10000000-1000]': 6,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Short of an accurate algorithm for computing this, you, noble reader, are  \n",
    "tasked with the mind-numbing task of looking at graphs and picking numbers.  \n",
    "This is an iterative process. You'll likely lose the will to live mid-way.  \n",
    "\n",
    "Persevere, friend. Your sacrifice will not go unrewarded.\n",
    "\n",
    "You'll be shown a plot of iteration vs execution time for each benchmark. An  \n",
    "x-axis intercept marks the current value of the first stable index.  \n",
    "\n",
    "You'll then be prompted to enter a new first stable index for each benchmark  \n",
    "until you arrive at a fixed point. Press Enter at the prompt to skip the current  \n",
    "benchmark. Press Ctrl+C at any time to give up.  \n",
    "\n",
    "Note that some benchmarks never really reach stability. In this case try to  \n",
    "pick a value that compromises between cost and  accuracy (ie if a benchmark is  \n",
    "really slow, we don't want to be doing tons of burn in iterations, whereas for  \n",
    "a fast benchmark we could justify more).  \n",
    "\n",
    "At the end, the first_stable_index dict will be printed. Please commit values  \n",
    "for new benchmarks or if your estimates differ from mine.  \n",
    "\n",
    "Good luck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ht = hl.read_table(f'{prefix}/out/benchmarks.ht')\n",
    "names: list[str] = ht.aggregate(hl.agg.collect_as_set(ht.name))  # type: ignore\n",
    "names = sorted(names)\n",
    "\n",
    "while len(names) != 0:\n",
    "    __new_names, names = names, []\n",
    "    for fig in plot_iteration_against_time(ht, __new_names, first_stable_index):\n",
    "        clear_output(wait=True)\n",
    "        pio.renderers.default = 'notebook'\n",
    "\n",
    "        name: str = fig.labels.title  # type: ignore\n",
    "        cur_index = first_stable_index.get(name)\n",
    "\n",
    "        try:\n",
    "            fig.show()\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            new_index = maybe(int, input('Enter the first stable index (or blank keep same)') or None)\n",
    "            if new_index is not None and new_index != cur_index:\n",
    "                first_stable_index[name] = new_index\n",
    "                names.append(name)\n",
    "        except KeyboardInterrupt as _:\n",
    "            break\n",
    "\n",
    "first_stable_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before conducting our analysis, we need to clean our data. That means we exclude\n",
    "- burn-in iterations\n",
    "- iterations that timed-out or otherwise failed\n",
    "- outliers with significant divergence from median execution time.\n",
    "- benchmarks that never reach a steady-state (maybe they should be deleted?)\n",
    "- instances that do not have sufficient iterations for analysis\n",
    "- benchmarks that do not have sufficient instances for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_burn_in_iterations(ht: hl.Table, first_stable_index: dict[str, int]) -> hl.Table:\n",
    "    ht = ht.annotate_globals(first_stable_index=first_stable_index)\n",
    "    return ht.select(\n",
    "        instances=ht.instances.map(\n",
    "            lambda instance: instance.annotate(\n",
    "                iterations=hl.filter(\n",
    "                    lambda t: t.idx >= ht.first_stable_index.get(ht.name, 0),\n",
    "                    annotate_index(instance.iterations),\n",
    "                ),\n",
    "            )\n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "def remove_outliers(ht: hl.Table, factor: hl.Float64Expression) -> hl.Table:\n",
    "    return ht.select(\n",
    "        instances=ht.instances.map(\n",
    "            lambda instance: instance.annotate(\n",
    "                iterations=hl.bind(\n",
    "                    lambda median: instance.iterations.filter(\n",
    "                        lambda t: hl.max([t.time, median]) / hl.min([t.time, median]) < factor\n",
    "                    ),\n",
    "                    hl.median(instance.iterations.map(lambda t: t.time)),\n",
    "                )\n",
    "            ),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "def keep_names(ht: hl.Table, names: hl.SetExpression) -> hl.Table:\n",
    "    return ht.filter(names.contains(ht.name))\n",
    "\n",
    "\n",
    "def remove_failed_iterations(ht: hl.Table) -> hl.Table:\n",
    "    return ht.annotate(\n",
    "        instances=ht.instances.map(\n",
    "            lambda i: i.annotate(\n",
    "                iterations=hl.filter(\n",
    "                    lambda t: ~t.timed_out | hl.is_missing(t.failure),\n",
    "                    i.iterations,\n",
    "                ),\n",
    "            )\n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "def remove_non_viable_instances(\n",
    "    ht: hl.Table,\n",
    "    min_instances: hl.Int32Expression,\n",
    "    min_iterations: hl.Int32Expression,\n",
    ") -> hl.Table:\n",
    "    ht = ht.annotate(\n",
    "        instances=hl.filter(\n",
    "            lambda instance: hl.len(instance.iterations) >= min_iterations,\n",
    "            ht.instances,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    return ht.filter(hl.len(ht.instances) >= min_instances)\n",
    "\n",
    "\n",
    "ht = hl.read_table(f'{prefix}/out/benchmarks.ht')\n",
    "all: list[str] = ht.aggregate(hl.agg.collect_as_set(ht.name))  # type: ignore\n",
    "\n",
    "ht = keep_names(ht, hl.set([n for n in all if n in first_stable_index]))\n",
    "ht = remove_burn_in_iterations(ht, first_stable_index)\n",
    "ht = remove_failed_iterations(ht)\n",
    "ht = remove_outliers(ht, hl.float64(10))\n",
    "ht = remove_non_viable_instances(ht, hl.int(50), hl.int(50))\n",
    "\n",
    "ht = ht.checkpoint(f'{prefix}/out/filtered.ht', overwrite=True)\n",
    "\n",
    "benchmarks = ht.aggregate(hl.agg.collect_as_set(ht.name))\n",
    "\n",
    "print('Filtered:', *(n for n in all if n not in set(benchmarks)), sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark Variability\n",
    "\n",
    "The next cells concern themselves with examining the variability of benchmarks,  \n",
    "both on a per-instance basis as well as total.\n",
    "\n",
    "The first cell plots mean execution time per instance to look for distinct modes.  \n",
    "If present and significant, it may be harder to identify performance differences  \n",
    "for this benchmark between instances.\n",
    "\n",
    "The second cell quantifies variability by computing the total and per-instance  \n",
    "coefficient of variation, defined as the ratio of stdev and mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ht = hl.read_table(f'{prefix}/out/filtered.ht')\n",
    "for f in plot_mean_time_per_instance(ht):\n",
    "    clear_output(wait=True)\n",
    "    pio.renderers.default = 'notebook'\n",
    "    f.show()\n",
    "    input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ht = hl.read_table(f'{prefix}/out/filtered.ht')\n",
    "ht = ht.select(instances=ht.instances.iterations.time)\n",
    "ht.select(**variability(ht)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detecting Slowdowns\n",
    "\n",
    "In the following section, we'll see what configurations are required to reliably  \n",
    "detect slowdowns via two methods: that of Laaber at al. and another of Patrick's  \n",
    "devising. See the comments in the source code for details of each.\n",
    "\n",
    "In a later section, we'll use these results to select configurations that minimise  \n",
    "false positives and maximise the minimal detectable slowdown, allowing for cost  \n",
    "and time.\n",
    "\n",
    "The analyses are fairly computationally intensive and I recommend switching to  \n",
    "the `batch` backend for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional - switch to the batch backend for MDS computations\n",
    "hl.stop()\n",
    "hl.init(backend='batch')\n",
    "\n",
    "new_prefix = hl.current_backend().remote_tmpdir\n",
    "hl.current_backend().fs.copy(f'{prefix}/out/filtered.ht', f'{new_prefix}/out/filtered.ht')\n",
    "prefix, new_prefix = new_prefix, prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ht = hl.read_table(f'{prefix}/out/filtered.ht')\n",
    "ht = ht.select(instances=ht.instances.iterations.time)\n",
    "\n",
    "laaber_mds(ht).write(f'{prefix}/out/laaber-mds.ht')\n",
    "schultz_mds(ht).write(f'{prefix}/out/schultz-mds.ht')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional - switch back to spark for plotting and localise the results\n",
    "for mds in ['laaber', 'schultz']:\n",
    "    hl.current_backend().fs.copy(f'{prefix}/out/{mds}-mds.ht', f'{new_prefix}/out/{mds}-mds.ht')\n",
    "\n",
    "prefix, new_prefix = new_prefix, prefix\n",
    "hl.stop()\n",
    "hl.init(backend='spark')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark Configurations\n",
    "\n",
    "Now that we've computed the slowdowns detectable by each configuration, we need  \n",
    "to select a configuration that reduces\n",
    "- the rate of false positives\n",
    "- the size of the change we can detect\n",
    "\n",
    "The end result of this work is to produce a configuration file for automating  \n",
    "benchmarking in such a way as to reliably detect slowdowns.\n",
    "\n",
    "Starting with slowdown a simulated slowdown of 1, you'll pick a configuration of  \n",
    "`ninstances, niterations` where the likelihood of detecting a change is as close  \n",
    "to zero as possible. I'm assuming that this acts as a threshold configuration,  \n",
    "after which configuration are no more likely to detect false positives.\n",
    "\n",
    "Removing all configurations less than this threshold, you'll then scan the  \n",
    "detectable slowdowns for configurations whose rate of detection of a given  \n",
    "slowdown is as close as possible to 1. For some benchmarks, it may be impossible  \n",
    "to detect small slowdowns. Enter your configuration in the prompt, leave the   \n",
    "prompt empty to skip or hit Ctrl+C to give up.\n",
    "\n",
    "At the end, a yaml document will be printed to the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "t = hl.read_table(f'{prefix}/out/filtered.ht')\n",
    "t = t.select(instances=t.instances.iterations.time)\n",
    "t = t.select(\n",
    "    mean=t.instances.aggregate(lambda k: hl.agg.explode(hl.agg.mean, k)),\n",
    "    cv=variability(t).total,\n",
    ")\n",
    "\n",
    "benchmarks = t.aggregate(hl.agg.collect((t.path, t.name, t.mean, t.cv)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laaber, schultz = [\n",
    "    (\n",
    "        t := hl.read_table(f'{prefix}/out/{m}-mds.ht'),\n",
    "        t := t._key_by_assert_sorted('path', 'name', 'slowdown', 'ninstances', 'niterations'),\n",
    "        t.select('ibs', 'tbs'),\n",
    "    )[-1]\n",
    "    for m in ('laaber', 'schultz')\n",
    "]\n",
    "\n",
    "mds = laaber.select(laaber=laaber.row_value, schultz=schultz[laaber.key])\n",
    "\n",
    "results: list[dict] = []\n",
    "\n",
    "for path, name, rt, cv in benchmarks:\n",
    "    try:\n",
    "        cur = {\n",
    "            'item': f'{path}::{name}',\n",
    "            'burn_in': first_stable_index[name],\n",
    "            'mean': float(f'{rt:.3f}'),\n",
    "            'cv': float(f'{cv:.3f}'),\n",
    "        }\n",
    "\n",
    "        info = Pretty(yaml.dump(data=[cur], sort_keys=False))\n",
    "\n",
    "        display(info, clear=True)\n",
    "\n",
    "        # The user will first pick a threshold configuration that minimises false\n",
    "        # positives.\n",
    "        t = mds.filter((mds.path == path) & (mds.name == name))\n",
    "        t.filter(t.slowdown == 1).show(100_000)\n",
    "\n",
    "        # Be fault-tolerant - if the user makes a typo then try again\n",
    "        skip = False\n",
    "        while True:\n",
    "            config = input(\n",
    "                'Enter `ninstances, niterations` that minimises the likelihood of detecting a false positive.'\n",
    "            )\n",
    "\n",
    "            if not config:\n",
    "                skip = True\n",
    "                break\n",
    "\n",
    "            try:\n",
    "                m, n = [int(x.strip()) for x in config.split(',')]\n",
    "                break\n",
    "            except Exception as _:\n",
    "                pass\n",
    "\n",
    "        if skip:\n",
    "            continue\n",
    "\n",
    "        display(info, clear=True)\n",
    "\n",
    "        # Now the user will scan the table for configurations greater than this\n",
    "        # threshold that can reliably detect the smallest slowdown, ie the rate of\n",
    "        # detection is as close as possible to 1\n",
    "\n",
    "        t.filter((t.slowdown > 1) & (hl.tuple([t.ninstances, t.niterations]) >= (m, n))).show(100_000)\n",
    "\n",
    "        # again with the fault-tolerance.\n",
    "        while True:\n",
    "            config = input(\n",
    "                'Enter `slowdown, ninstances, niterations` that maximises the likelihood of detecting a slowdown.'\n",
    "            )\n",
    "\n",
    "            if not config:\n",
    "                break\n",
    "\n",
    "            try:\n",
    "                slowdown, m, n = [x.strip() for x in config.split(',')]\n",
    "                results.append({\n",
    "                    **cur,\n",
    "                    'config': [\n",
    "                        {\n",
    "                            'slowdown': float(slowdown),\n",
    "                            'instances': int(m),\n",
    "                            'iterations': int(n),\n",
    "                        }\n",
    "                    ],\n",
    "                })\n",
    "                break\n",
    "            except Exception as _:\n",
    "                pass\n",
    "\n",
    "    except KeyboardInterrupt as _:\n",
    "        break\n",
    "\n",
    "# print as yaml because everything needs to be yaml because yaml is yaml\n",
    "display(Pretty(yaml.dump(data=results, sort_keys=False)), clear=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

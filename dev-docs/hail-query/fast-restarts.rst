====================================
CollectDistributedArray Call-Caching
====================================

.. role:: scala(code)


Introduction
==========
See `Fast Restarts for Failed Queries <https://github.com/ehigham/hail-rfc/tree/ehigham/call-caching-proposal>`_.

Proposed Change Specification
=============================

Experience tells us that the majority of time spent on expensive and
scientifically interesting queries is within the tasks generated by
:scala:`CollectDistributedArray` (:scala:`CDA`).
This is because many of the table operations in hail's :scala:`IR` are lowered
into one or more :scala:`CDA` operations.
Consequently, we focus our attention on caching the intermediate results of
these tasks.

:scala:`CDA` can be thought of as a distributed map-reduce operation, from some
input "context" for each partition in a table (eg, the path to the file
where the partition is serialised), a computation on that partition, and some
combiner for the results of those computations.
For what follows, let an *activation* be a particular invocation of a
:scala:`CDA` pipeline (implemented via :scala:`collectDArray`).

At a high-level, when the driver performs an *activation*, it will look in its
*execution cache* to see if it had successfully performed that *activation*
in the past.
The *cache* contains the results for all the successful partition computations.
The driver compares the tasks for each partition with the results in the cache
and removes those tasks that have already been completed.
It then executes any remaining work and updates the execution cache with their
results.
If all the work completes successfully, the driver returns the now-cached
results to be used in the the rest of the query.
The driver will cache the results of successful *activations* only.
Failed *activations* (ie. those that errored) will be handled in the usual way,
potentially failing the query.

We require two things to determine if the driver had successfully executed an
operation:

1. a way of looking up *activations* in a *cache*, and
2. then design of the execution cache itself

Semantic Hashing
----------------
To lookup operations in the cache, we need a way of producing an identifier
that uniquely represents a particular *activation*.
We do this by defining a *semantic hash* for the activation, comprised of:

a) a *static* component computed from the :scala:`IR` that generated the
   operation
b) a *dynamic* component for the particular activation instance.

For most :scala:`IR` nodes, the *static* component can be computed purely from
their inputs plus some contribution uniquely representing the semantics of that
class of :scala:`IR`.
For :scala:`IR` nodes that read external files, we have to be a little more
cautious and ensure that those files haven't changed since we last read them.
Thus, we need to include some kind of checksum or digest of that file.
This static component can be passed down the lowering pipeline to the code
generator and driver, which, when performing an activation, can mix the static
component with a dynamically generated activation id to form the semantic hash.

Execution Cache
---------------

Users will "bring their own"\ :sup:`TM` cache directory where cached
computations will be stored.
This cache dir will be an prefix in local or cloud storage.
The driver will store cache files named ``{cachedir}/{semhash}``, where

- `cachdir` is a user-defined location, defaulting to
  `{tmp}/hail/{hail-pip-version}`
- `tmp` is either the local tempdir for spark and local backends, or the
  remote  tempdir for `QoB`.

These files will contain accumulated activation results, indexed by their
partition number.

Examples
========

To opt in or out of fast-restarts, users will set hail flags in their python
client:

..  code-block:: python

    >> hl._set_flags(use_fast_restarts='1')
    >> hl._set_flags(cachedir='gs://my-bucket/cache/0')


Alternatively, users can set the corresponding environment variables at the
command line prior to starting their python session:

..  code-block:: sh

    >> HAIL_USE_FAST_RESTARTS=1 HAIL_CACHE_DIR='gs://my-bucket/cache/0' ipython

Notes:

- The definition of the ``cachedir`` does not imply
  ``use_fast_restarts``.
- If ``use_fast_restarts`` is defined, hail will write cache entries to
  a subfolder of the ``tmpdir`` by default.


Implementation Description
==========================

The reader should note that implementation examples below are for illustrative
purposes only and that the real implementation may differ slightly.

Semantic Hashes
---------------

Computing Static Component
^^^^^^^^^^^^^^^^^^^^^^^^^^

We can compute the static component of a semantic hash for the :code:`IR` in
a level-order traversal of the nodes in the :code:`IR`.
Node that the actual order doesn't matter, just that an order is defined.

Since the ``IR`` contains references and compiler-generated names, we need to
normalise the names in the :code:`IR` (see :scala:`NormalizeNames.scala`)
to get consistent hashes.

The semantic hash is defined for the whole :code:`IR` (as apposed to prefixes
of the :code:`IR` tree, see Alternatives below).
Thus, we'll compute the hash as early as possible to minimise the computational
cost as the :scala:`IR` gets lowered and expanded.
This also reduces the number of :code:`BaseIR` operations we need to define
semantic hashes for (ie. only those that can be constructed in python).


..  code-block:: scala

    object LevelOrder {
      def apply(root: BaseIR): Iterator[BaseIR]
    }

Then, assuming we have an appropriate hashing algorithm, seed and a way of
combining hashes:

..  code-block:: scala

    @newtype case class Hash(v: ???) extends AnyVal {
      def <>(b: Hash): Hash
    }

    val seed: Hash = ???
    def hash(a: Any): Hash = ???


Then:

..  code-block:: scala

    object SemanticHash {
      def apply(fs: FS, root: BaseIR): Option[Hash] =
        LevelOrder(NormalizeNames(root)).foldLeft(seed) { (s, ir) =>
          s <> ir match {
            case Ref(name, _) =>
              hash(classOf[Ref]) <> hash(name)

            case TableRead(_, _, reader) =>
              hash(classOf[TableRead]) <> reader
                .pathsUsed
                .map(fs.etag)
                .foldLeft(hash(reader.getClass))(_ <> hash(_))

            case ir if DependsOnlyOnInputs(ir) =>
              Hash(ir.getClass)

            case _ if DontKnowHowToDefineSemhash(ir) =>
               return None

            case ... =>
          }
        }
    }

Observations:

- For all :code:`IR` nodes that depend only on their children and have no
  additional parameterisation, their semantic hash is simply some unique
  encoding for what that node means.
- Hashing :code:`IR`'s class is sufficient
- Note that the node's children will be hashed in the traversal
- There are times when we can't define a semantic hash (such as reading a
  table from a RVD). In these cases, we'll just return :scala:`None`.


Computing Dynamic Component
^^^^^^^^^^^^^^^^^^^^^^^^^^^

The query driver is a single-threaded system that compiles and executes the
same queries in a repeatable way.
That is, if a query generates one or more :code:`CDA` nodes, those nodes will be
emitted in the same order.
This, we can use the static component in the same way as random number
generator state:

- When a :scala:`CDA` node is emitted, we can fork the semhash key-value
- We "mix" one value with the :code:`CDA`'s dynamic id to generate the semantic
  hash for that particular activation
- and update the static component state with the forked value for the next
  :code:`CDA` node.

To do this, we can add the function :code:`nextHash` to the
:code:`ExecuteContext` that returns a new `Hash` value to be mixed with the
dynamic component and updates internal state:

..  code-block:: scala

    final case class IrMetadata(semhash: Option[Hash]) {
        private[this] var counter: Int = 0

        def nextHash: Option[Hash] = {
           val n = hash(counter)
           counter += 1
           semhash.map(_ <> n)
        }
    }

Then, in :scala:`Emit.scala`:

..  code-block:: scala

    case cda: CollectDistributedArray =>
      ...
      semhash <- cb.newLocal("semhash")
      emitI(dynamicID).consume(cb,
        (),
        nextHash.foreach { hash =>
          cb.assign(semhash, hash)
        },
        { dynamicID =>
            nextHash.foreach { staticHash =>
              val dynamicHash =
                Code.invokeScalaObject[Hash](
                  Hash.getClass,
                  "apply",
                  Array(classOf[String]),
                  Array(dynamicID.loadString(cb))
                )

              val combined =
                Code.invokeScalaObject[Hash](
                  Hash.getClass,
                  "<>",
                  Array.fill(2)(classOf[Hash]),
                  Array(staticHash, dynamicHash)
                )

              cb.assign(semhash, combined)
            }
        }
      )

      // call `collectDArray` with semhash

Using :code:`Option` allows us to encode if we can compute a semantic hash
for the given :code:`IR`.
In the case when one cannot be computed, :code:`collectDArray` simply skips
reading and updating a cache.


Alternatives
^^^^^^^^^^^^

The following describes a means of computing and assigning the static portion of
semantic hashes for each node in the :code:`IR`.
The aim was to support extending queries that were developed incrementally and
interactively by recognising and caching query prefixes.
When the compiler sees a prefix of the query that it had already computed, it
would simply load the result from the cache rather than recompute.
In reality this is very hard to do as :code:`PruneDeadFields` changes the
semantics of the :code:`IR`, meaning the what's computed depends on how the
result is used.

We can compute the static component of a semantic hash from a bottom-up
traversal of the IR ``IR``.
Since the ``IR`` supports references, we need to compute a binding environment
top-down that maps names to their definitions, so we can look up the static
component of the value being referenced:

..  code-block:: scala

    type BindingEnv = Map[String, BaseIR]

    object FlattenTopDown {
      def apply(ir: BaseIR, env: BindingEnv): Iterator[(BaseIR, BindingEnv)] =
        ir match {
          case Let(name, value, body) =>
            FlattenTopDown(value, env) ++
            FlattenTopDown(body, env.put(name, value)) ++
            Iterator.single(ir, env)

          case ... =>
        }
    }

Then, assuming we have an appropriate hashing algorithm and a way of combining
hashes:

..  code-block:: scala

    def hash(a: Any): Hash = ???
    @newtype case class Hash(v: ???) {
      def <>(b: Hash): Hash = ???
    }

Then:

..  code-block:: scala

    object BottomUp {
      def apply(fs: FS, memo: Memo[Hash])(ir: BaseIR, env: BindingEnv): Hash =
        ir match {
          case Ref(name, _) =>
            hash(classOf[Ref]) <> memo(env(name))

          case TableRead(_, _, reader) =>
            reader
              .pathsUsed
              .map(fs.digest)
              .foldLeft(hash(classOf[TableRead]))(_ <> hash(_))

          case ir if DependsOnlyOnInputs(ir) =>
            ir.children.foldLeft(hash(ir.getClass))(_ <> memo(_))

          case ... =>
        }
    }

How to eliminate the effects of compiler-generated names?
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The compiler generates names for struct fields.
Thus, semantic hashes of struct expressions that use compiler-generated names in
the computation for the hash will not hash to the same value.
This is problematic as semantic-hashing is a forward data-flow computation -
different hashes upstream will cause the rest of the query to cache-miss,
despite being the same program.

One approach might be to maintain a record of every struct definition, mapping
field names to their definitions.
When we encounter a :scala:`GetField` expression, we look up the :scala:`IR`
that defined that field and fetch its semantic hash.

The problem with this is it assumes that every use of an
expression of type :scala:`TStruct` has a unique corresponding
:scala:`MakeStruct` definition.
This is not true in the :scala:`IR`, as that struct could be generated from
a read of a partition or from an empty stream of type :scala:`TStruct`, or from
many :scala:`MakeStruct` nodes.

Consider the following fragment:

..  code-block:: scala

    StreamMap(inputstream, "x",
      GetField(Ref("x", TStruct("__ruid_XXXX" -> TInt), "__ruid_XXXX"))
    )

In order to eliminate the compiler-generated name :scala:`"__ruid_XXXX"`, we
have to analyse through the reference :scala:`"x"`.
The :scala:`IR` doesn't define a binding for :scala:`"x"` statically, nor indeed
can it in the general case.
To illustrate this point, consider the two cases below:

1. more than one definition

..  code-block:: scala

    val typ = TStruct("__ruid_XXXX" -> TInt)
    val inputstream =
      MakeStream(
        MakeArray(
          Array(
            MakeStruct("__ruid_XXXX" -> I32(0)),
            MakeStruct("__ruid_XXXX" -> I32(1))
          ),
          TArray(typ)
        ),
        TStream(typ)
      )


Now, in our :scala:`StreamMap` example above, we cannot map
:scala:`"__ruid_XXXX"` to a unique definition.

2. no definitions

..  code-block:: scala

    val typ = TStruct("__ruid_XXXX" -> TInt)
    val inputstream =
      MakeStream(
        MakeArray(Array.empty, TArray(typ)),
        TStream(typ)
      )

Now, our :scala:`StreamMap` example will never execute. Is semantic hashing
meant to detect this and eliminate such expressions?


Execution Cache
---------------

Given an interface for an :scala:`ExecutionCache`` of the form:

..  code-block:: scala

    trait ExecutionCache {
      def lookup(h: SemanticHash): Array[(Int, Array[Byte])]
      def put(h: SemanticHash, res: Array[(Int, Array[Byte])]): Unit
    }

We can implement a file-system cache that uses a file prefix plus the current
version of Hail to generate a "root" directory, under which all cache files are
stored by their semantic hash.

An implementation might look as follows:

..  code-block:: scala

    final case class FSExecutionCache(fs: FS, cachedir: String)
      extends ExecutionCache {

      override def lookup(h: SemanticHash): Array[(Int, Array[Byte])] =
        Using(fs.open(s"$cachedir/$h")) { _.split(newline).map(CacheLine.read) }
          .getOrElse(Array.empty)

      override def put(h: SemanticHash, res: Array[(Int, Array[Byte])]): Unit =
        fs.write(s"$cachedir/${HailContext.version}/$h") { ostream =>
          res.foreach { CacheLine.write(ostream) }
        }

        object CacheLine {
          def write(ostream: OutputStream): (Int, Array[Byte]) => Unit = {
            case (index, data) =>
              ostream.write(index)
              ostream.write(", ")
              ostream.write(Base64Encode(data))
              ostream.write(newline)
          }

          def read(s: String): (Int, Array[Byte]) = {
            val (index, s) = readInt(s)
            val (_, s) = readString(s, ", ")
            CacheLine(index, Base64Decode(s.getBytes))
          }
        }
    }

For testing, we can simply create a wrapper around a :scala:`mutable.HashMap`:

..  code-block:: scala

    @newtype case class MemExecutionCache(
        m: mutable.HashMap[SemanticHash, Array[(Int, Array[Byte])]]
    ) extends ExecutionCache { ... }

Endorsements
=============

.. (Optional) This section provides an opportunity for any third parties to express their
.. support for the proposal, and to say why they would like to see it adopted.
.. It is not mandatory for have any endorsements at all, but the more substantial
.. the proposal is, the more desirable it is to offer evidence that there is
.. significant demand from the community.  This section is one way to provide
.. such evidence.

FROM debian:9.5
MAINTAINER Hail Team <hail@broadinstitute.org>

USER root

# - libnlopt-dev is for nloptr, a dependency of SKAT:
#   https://github.com/jyypma/nloptr/issues/40
#
# - r-cran-ncdf4, r-cran-rnetcdf, libcurl4-openssl-dev, and libxml2-dve are
# - depenendencies for the three biolite packages
RUN apt-get update && \
  apt -y install gnupg2 lsb-release curl \
  curl -fsSL https://download.docker.com/linux/debian/gpg | apt-key add - && \
  echo \
   "deb [arch=amd64] https://download.docker.com/linux/debian stretch stable" \
    >>/etc/apt/sources.list && \
  apt-get update && \
  apt-get -y install \
    bash \
    cmake \
    g++ \
    git \
    libnlopt-dev \
    openjdk-8-jdk-headless \
    pandoc \
    r-base \
    tar \
    unzip \
    xsltproc \
    r-cran-ncdf4 \
    r-cran-rnetcdf \
    libcurl4-openssl-dev \
    libxml2-dev \
    wget \
    docker-ce \
    python3 && \
  rm -rf /var/lib/apt/lists/*

RUN wget https://repo.anaconda.com/miniconda/Miniconda2-4.5.4-Linux-x86_64.sh -O miniconda.sh && \
  /bin/bash miniconda.sh -b -p /opt/conda && \
  rm miniconda.sh
ENV PATH $PATH:/opt/conda/bin

RUN Rscript -e 'install.packages(c("jsonlite", "SKAT", "logistf"), repos = "http://cran.us.r-project.org")' && \
    Rscript -e 'source("https://bioconductor.org/biocLite.R"); biocLite("GENESIS"); biocLite("SNPRelate"); biocLite("GWASTools")'

# this seems easier than getting the keys right for apt
#
# source: https://cloud.google.com/storage/docs/gsutil_install#linux
RUN /bin/sh -c 'curl https://sdk.cloud.google.com | bash' && \
    mv /root/google-cloud-sdk /
ENV PATH $PATH:/google-cloud-sdk/bin

RUN wget -O  /usr/local/bin/kubectl https://storage.googleapis.com/kubernetes-release/release/v1.11.3/bin/linux/amd64/kubectl && \
    chmod +x /usr/local/bin/kubectl

WORKDIR /spark
RUN wget -O spark-2.2.0-bin-hadoop2.7.tgz https://archive.apache.org/dist/spark/spark-2.2.0/spark-2.2.0-bin-hadoop2.7.tgz && \
    tar --strip-components=1 -xvzf spark-2.2.0-bin-hadoop2.7.tgz -C /spark && \
    rm -rf spark-2.2.0-bin-hadoop2.7.tgz
ENV SPARK_HOME /spark/

WORKDIR /plink
RUN wget -O plink_linux_x86_64.zip http://www.cog-genomics.org/static/bin/plink/plink_linux_x86_64.zip && \
    unzip plink_linux_x86_64.zip && \
    rm -rf plink_linux_x86_64.zip && \
    ln -s /plink/plink /bin/plink

WORKDIR /qctool
RUN wget -O qctool_v2.0.1-CentOS6.8-x86_64.tgz http://www.well.ox.ac.uk/~gav/resources/qctool_v2.0.1-CentOS6.8-x86_64.tgz && \
    tar --strip-components=1 -xvzf qctool_v2.0.1-CentOS6.8-x86_64.tgz -C /qctool && \
    rm -rf qctool_v2.0.1-CentOS6.8-x86_64.tgz && \
    ln -s /qctool/qctool /bin/qctool

# creates /hail
WORKDIR /hail
RUN mkdir -p /gradle-cache

COPY hail/gradlew hail/build.gradle hail/settings.gradle hail/deployed-spark-versions.txt ./
COPY hail/gradle gradle
COPY hail/python/hail/dev-environment.yml python/hail/dev-environment.yml
COPY batch/environment.yml batch/environment.yml
COPY ci/environment.yml ci/environment.yml

RUN useradd --create-home --shell /bin/bash hail && \
    chown -R hail:hail /hail /gradle-cache
USER hail

# cache (almost all) gradle dependencies (and gradle itself)
#
# Some plugin we use hides its dependencies in a so-called "detached
# configuration" so there is no way for us to ensure it gets cached.
RUN ./gradlew downloadDependencies --gradle-user-home /gradle-cache
RUN conda env create -f ./python/hail/dev-environment.yml && \
    conda env create -f batch/environment.yml && \
    conda env create -f ci/environment.yml && \
    rm -rf /home/hail/.conda/pkgs/*

# gcloud iam key files will be stored here
VOLUME /secrets

WORKDIR /hail

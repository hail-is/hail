buildscript {
    repositories {
        mavenCentral()
        jcenter()
    }
    dependencies {
        classpath 'com.github.jengelman.gradle.plugins:shadow:1.2.3'
    }
}

import com.github.jengelman.gradle.plugins.shadow.tasks.ShadowJar

apply plugin: 'java'
apply plugin: 'scala'
apply plugin: 'idea'
apply plugin: 'application'
apply plugin: 'maven'
apply plugin: 'jacoco'
apply plugin: 'com.github.johnrengelman.shadow'

repositories {
    mavenCentral()
    jcenter()
    maven {
        url "https://repository.cloudera.com/artifactory/cloudera-repos/" // kudu
    }
}

mainClassName = "org.broadinstitute.hail.driver.Main"

String scalaVersion = '2.10.4'
String scalaMajorVersion = '2.10'
String sparkVersion = System.getProperty("hail.sparkversion","1.5.0")
// String scalaVersion = '2.11.7'
// String scalaMajorVersion = '2.11'

sourceSets.main.scala.srcDir "src/main/java"
sourceSets.main.java.srcDirs = []

compileScala {
    scalaCompileOptions.additionalParameters = ["-feature"]
    // scalaCompileOptions.forkOptions.jvmArgs = ['-XX:MaxPermSize=512m']
}

dependencies {
    compile 'org.scala-lang:scala-library:' + scalaVersion
    // compile 'org.apache.hadoop:hadoop-aws:2.7.1'
    compile('org.apache.spark:spark-core_' + scalaMajorVersion + ':' + sparkVersion) {
        exclude module: 'hadoop-client'
    }
    compile('org.apache.hadoop:hadoop-client:2.7.1') {
        exclude module: 'servlet-api'
    }
    // compile 'org.apache.spark:spark-core_' + scalaMajorVersion + ':1.5.0'
    compile 'org.apache.spark:spark-sql_' + scalaMajorVersion + ':' + sparkVersion
    compile 'org.apache.spark:spark-mllib_' + scalaMajorVersion + ':' + sparkVersion
    compile 'net.jpountz.lz4:lz4:1.3.0'
    compile 'org.apache.commons:commons-math3:3.5'
    compile 'org.apache.commons:commons-lang3:3.4'
    compile 'org.scalanlp:breeze_' + scalaMajorVersion + ':0.11.2'
    compile 'args4j:args4j:2.32'
    compile 'com.github.samtools:htsjdk:2.5.0'
    compile 'org.kududb:kudu-client:0.9.0'
    compile 'org.kududb:kudu-spark_2.10:0.9.0'
    compile('org.seqdoop:hadoop-bam:7.6.0') {
        exclude group: 'org.apache.hadoop'
        exclude module: 'htsjdk'
    }

    compile 'org.json4s:json4s-core_2.10:3.2.10'
    // compile 'org.json4s:json4s-native_2.10:3.2.10'
    compile 'org.json4s:json4s-jackson_2.10:3.2.10'
    compile 'org.json4s:json4s-ast_2.10:3.2.10'

    compile 'org.apache.solr:solr-solrj:5.5.0'
    compile 'com.datastax.cassandra:cassandra-driver-core:3.0.0'

    testCompile 'org.testng:testng:6.8.21'
    testCompile 'org.scalatest:scalatest_' + scalaMajorVersion + ':2.2.4'
}

test {
    useTestNG {}

    Random random = new Random()

    def randomize = System.getProperty("hail.randomize","false").toBoolean()
    def randomSeed = random.nextInt()

    if (randomize) {System.setProperty("hail.seed", randomSeed.toString())
    } else if (System.getProperty("hail.seed") == null) {
        System.setProperty("hail.seed", "1")
    }
    def seed = System.getProperty("hail.seed")
    println "Using a seed of [$seed] for testing."

    systemProperties System.getProperties()

    testLogging {
        events "passed", "skipped", "failed"
    }

    // listen to events in the test execution lifecycle
    beforeTest { descriptor ->
        logger.lifecycle("Running test: " + descriptor)
    }
}

tasks.withType(ShadowJar) {
    manifest {
        attributes 'Implementation-Title': 'Hail',
                'Implementation-Version': '0.0.1-SNAPSHOT',
                'Main-Class': 'org.broadinstitute.hail.driver.Main'
    }
    baseName = project.name + '-all'
    mergeServiceFiles()
    zip64 true
    // conflict with version of math3 in default Hadoop/Spark install
    relocate 'org.apache.commons.math3', 'org.broadinstitute.hail.relocated.org.apache.commons.math3'
}

shadowJar {
    classifier = 'spark'
    from(project.sourceSets.main.output)
    configurations = [project.configurations.runtime]
    dependencies {
        include(dependency('net.jpountz.lz4:lz4:.*'))
        include(dependency('org.apache.commons:commons-math3:.*'))
        include(dependency('org.scalanlp:breeze_' + scalaMajorVersion + ':.*'))
        include(dependency('args4j:args4j:.*'))
        include(dependency('com.github.samtools:htsjdk:.*'))
        include(dependency('org.seqdoop:hadoop-bam:.*'))

        include(dependency('org.json4s:json4s-core_2.10:.*'))
        // include(dependency('org.json4s:json4s-native_2.10:.*'))
        include(dependency('org.json4s:json4s-jackson_2.10:.*'))
        include(dependency('org.json4s:json4s-ast_2.10:.*'))

        include(dependency('org.apache.solr:solr-solrj:.*'))
        include(dependency('com.datastax.cassandra:cassandra-driver-core:.*'))
        include(dependency('org.kududb:kudu-client:.*'))
        include(dependency('org.kududb:kudu-spark_2.10:.*'))
    }
}

task shadowTestJar(type: ShadowJar) {
    classifier = 'spark-test'
    from(project.sourceSets.main.output, project.sourceSets.test.output)
    configurations = [project.configurations.testRuntime]
    dependencies {
        include(dependency('net.jpountz.lz4:lz4:.*'))
        include(dependency('org.apache.commons:commons-math3:.*'))
        include(dependency('org.scalanlp:breeze_' + scalaMajorVersion + ':.*'))
        include(dependency('args4j:args4j:.*'))
        include(dependency('com.github.samtools:htsjdk:.*'))
        include(dependency('org.seqdoop:hadoop-bam:.*'))

        include(dependency('org.json4s:json4s-core_2.10:.*'))
        // include(dependency('org.json4s:json4s-native_2.10:.*'))
        include(dependency('org.json4s:json4s-jackson_2.10:.*'))
        include(dependency('org.json4s:json4s-ast_2.10:.*'))

        include(dependency('org.testng:testng:.*'))
        include(dependency('com.beust:jcommander:.*'))
        include(dependency('org.scalatest:scalatest_' + scalaMajorVersion + ':.*'))

        include(dependency('org.apache.solr:solr-solrj:.*'))
        include(dependency('com.datastax.cassandra:cassandra-driver-core:.*'))
        include(dependency('org.kududb:kudu-client:.*'))
        include(dependency('org.kududb:kudu-spark_2.10:.*'))
    }
}

jacocoTestReport {
    dependsOn test
    reports {
        xml.enabled false
        csv.enabled false
        html.destination "${buildDir}/reports/coverage"
    }
}

task coverage(dependsOn: jacocoTestReport)

task testJar(type: Jar) {
    classifier = 'tests'
    from sourceSets.test.output
}

sourceCompatibility = 1.7
targetCompatibility = 1.7

compileScala {
    scalaCompileOptions.useAnt = false
}
